#!/usr/bin/env python3
"""
èªæ„åˆ‡å‰²ç³»çµ±æ¸¬è©¦å·¥å…·

é©—è­‰èªæ„åˆ‡å‰²çš„æ•ˆæœï¼Œæ¯”è¼ƒæ–°èˆŠåˆ‡å‰²æ–¹å¼çš„å·®ç•°
"""

import asyncio
import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src" / "main" / "python"))

from dotenv import load_dotenv
load_dotenv()

from src.main.python.rag.semantic_chunking import SemanticChunker, ChunkingConfig
from src.main.python.rag.enhanced_vector_store import EnhancedVectorStore
from src.main.python.rag.chunking_config import ConfigManager, create_default_config_file


async def test_semantic_chunking_basic():
    """åŸºç¤èªæ„åˆ‡å‰²æ¸¬è©¦"""
    print("=== åŸºç¤èªæ„åˆ‡å‰²æ¸¬è©¦ ===\n")

    # æ¸¬è©¦æ–‡æœ¬ï¼ˆæ¨¡æ“¬è²¡ç¶“æ–°èï¼‰
    test_text = """
    å°ç©é›»å…¬å¸ƒç¬¬ä¸‰å­£è²¡å ±ï¼Œç‡Ÿæ”¶é”æ–°å°å¹£6,136å„„å…ƒï¼Œè¼ƒå‰å­£æˆé•·14.6%ï¼Œè¼ƒå»å¹´åŒæœŸæˆé•·16.9%ã€‚
    æ¯›åˆ©ç‡ç‚º54.3%ï¼Œç‡Ÿæ¥­åˆ©ç›Šç‡ç‚º45.1%ï¼Œç¨…å¾Œæ·¨åˆ©ç‚º1,548å„„å…ƒï¼Œæ¯è‚¡ç›ˆé¤˜ç‚º5.97å…ƒã€‚

    ç„¶è€Œï¼Œæ³•äººåˆ†æå¸«å°æ–¼æ˜å¹´ç¬¬ä¸€å­£çš„å±•æœ›è¼ƒç‚ºä¿å®ˆã€‚
    ä¸»è¦é—œæ³¨é»åŒ…æ‹¬æ™ºæ…§å‹æ‰‹æ©Ÿéœ€æ±‚æ”¾ç·©ã€åœ°ç·£æ”¿æ²»é¢¨éšªæŒçºŒï¼Œä»¥åŠå…¨çƒç¶“æ¿Ÿæˆé•·è¶¨ç·©çš„å½±éŸ¿ã€‚

    æŠ€è¡“é¢åˆ†æé¡¯ç¤ºï¼Œå°ç©é›»è‚¡åƒ¹åœ¨500å…ƒé™„è¿‘é‡åˆ°é˜»åŠ›ã€‚
    æˆäº¤é‡è¼ƒå‰ä¸€å€‹äº¤æ˜“æ—¥æ¸›å°‘15%ï¼Œé¡¯ç¤ºæŠ•è³‡äººæŒè§€æœ›æ…‹åº¦ã€‚
    è‹¥èƒ½çªç ´510å…ƒé—œå¡ï¼Œä¸‹ä¸€å€‹ç›®æ¨™åƒ¹ä½å¯èƒ½è½åœ¨530å…ƒã€‚

    æ­¤å¤–ï¼Œå°ç©é›»å®£å¸ƒå°‡åœ¨ç¾åœ‹äºåˆ©æ¡‘é‚£å·èˆˆå»ºç¬¬äºŒåº§æ™¶åœ“å» ã€‚
    é è¨ˆ2025å¹´é–‹å§‹é‡ç”¢ï¼ŒåˆæœŸæœˆç”¢èƒ½ç‚º2è¬ç‰‡12å‹æ™¶åœ“ã€‚
    é€™é …æŠ•è³‡å°‡æœ‰åŠ©æ–¼å°ç©é›»åˆ†æ•£åœ°ç·£æ”¿æ²»é¢¨éšªï¼Œå¼·åŒ–ä¾›æ‡‰éˆå½ˆæ€§ã€‚
    """

    # å‰µå»ºé…ç½®
    config = ChunkingConfig(
        min_chunk_size=150,
        max_chunk_size=500,
        target_chunk_size=300,
        similarity_threshold=0.7,
        boundary_confidence_threshold=0.5,
        enable_financial_optimization=True
    )

    # åˆå§‹åŒ–åˆ‡å‰²å™¨
    chunker = SemanticChunker(config)

    try:
        print("åŸ·è¡Œèªæ„åˆ‡å‰²...")
        start_time = time.time()
        chunks = await chunker.chunk_text(test_text)
        processing_time = time.time() - start_time

        print(f"åˆ‡å‰²å®Œæˆï¼Œè€—æ™‚: {processing_time:.2f}ç§’")
        print(f"ç¸½å…±ç”¢ç”Ÿ {len(chunks)} å€‹åˆ‡å‰²ç‰‡æ®µ\n")

        # åˆ†æåˆ‡å‰²çµæœ
        for i, chunk in enumerate(chunks, 1):
            print(f"--- ç‰‡æ®µ {i} ---")
            print(f"é•·åº¦: {len(chunk.text)} å­—ç¬¦")
            print(f"å¥å­æ•¸: {chunk.metadata.get('sentence_count', 'N/A')}")
            print(f"é‚Šç•Œä¿¡å¿ƒåº¦: {chunk.boundary_confidence:.3f}")
            print(f"èªæ„ä¸€è‡´æ€§: {chunk.semantic_coherence:.3f}")
            print(f"é‡ç–Šé•·åº¦: {chunk.overlap_length}")
            print(f"å…§å®¹é è¦½: {chunk.text[:100]}...")
            print()

        return chunks

    except Exception as e:
        print(f"æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return []


async def test_vs_legacy_chunking():
    """å°æ¯”æ¸¬è©¦ï¼šèªæ„åˆ‡å‰² vs å‚³çµ±åˆ‡å‰²"""
    print("=== èªæ„åˆ‡å‰² vs å‚³çµ±åˆ‡å‰²å°æ¯” ===\n")

    # è¼‰å…¥æ¸¬è©¦æ•¸æ“š
    test_data_path = project_root / "data" / "real_cnyes_news_20250927_132114.json"

    if not test_data_path.exists():
        print(f"æ¸¬è©¦æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {test_data_path}")
        print("å°‡ä½¿ç”¨æ¨¡æ“¬æ•¸æ“šé€²è¡Œæ¸¬è©¦")

        test_articles = [
            "å°ç©é›»ç¬¬ä¸‰å­£ç‡Ÿæ”¶å‰µæ–°é«˜ï¼Œå—æƒ æ–¼è˜‹æœiPhone 15ç³»åˆ—éœ€æ±‚å¼·å‹ã€‚ç„¶è€Œåˆ†æå¸«æ“”å¿ƒæ˜å¹´æ™¯æ°£ä¸ç¢ºå®šæ€§ã€‚",
            "è¯ç™¼ç§‘ç™¼å¸ƒæ–°ä¸€ä»£5Gæ™¶ç‰‡ï¼Œæ•ˆèƒ½æå‡30%ã€‚é è¨ˆå°‡æ¶æ”»ä¸­é«˜éšæ‰‹æ©Ÿå¸‚å ´ï¼Œèˆ‡é«˜é€šå±•é–‹æ¿€çƒˆç«¶çˆ­ã€‚",
            "å¤®è¡Œå®£å¸ƒç¶­æŒåˆ©ç‡ä¸è®Šï¼Œç¬¦åˆå¸‚å ´é æœŸã€‚ä½†æš—ç¤ºæœªä¾†å¯èƒ½æ¡å–æ›´ç©æ¥µçš„è²¨å¹£æ”¿ç­–å› æ‡‰é€šè†¨å£“åŠ›ã€‚"
        ]
    else:
        with open(test_data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        articles = data.get('articles', [])
        test_articles = [
            f"æ¨™é¡Œï¼š{article.get('title', '')}\nå…§å®¹ï¼š{article.get('content', '')}"
            for article in articles[:3]  # æ¸¬è©¦å‰3ç¯‡
        ]

    print(f"æ¸¬è©¦ {len(test_articles)} ç¯‡æ–‡ç« ")

    # èªæ„åˆ‡å‰²æ¸¬è©¦
    print("\n1. èªæ„åˆ‡å‰²çµæœ:")
    semantic_chunker = SemanticChunker(ChunkingConfig(
        min_chunk_size=200,
        max_chunk_size=600,
        enable_financial_optimization=True
    ))

    semantic_results = []
    semantic_total_time = 0

    for i, article in enumerate(test_articles, 1):
        print(f"\næ–‡ç«  {i} (é•·åº¦: {len(article)} å­—ç¬¦):")

        start_time = time.time()
        chunks = await semantic_chunker.chunk_text(article)
        processing_time = time.time() - start_time
        semantic_total_time += processing_time

        chunk_sizes = [len(chunk.text) for chunk in chunks]
        avg_coherence = sum(chunk.semantic_coherence for chunk in chunks) / len(chunks) if chunks else 0

        result = {
            'article_length': len(article),
            'chunk_count': len(chunks),
            'avg_chunk_size': sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0,
            'chunk_size_range': f"{min(chunk_sizes)}-{max(chunk_sizes)}" if chunk_sizes else "0-0",
            'avg_coherence': avg_coherence,
            'processing_time': processing_time
        }

        semantic_results.append(result)

        print(f"  åˆ‡å‰²ç‰‡æ®µæ•¸: {result['chunk_count']}")
        print(f"  å¹³å‡ç‰‡æ®µå¤§å°: {result['avg_chunk_size']:.0f} å­—ç¬¦")
        print(f"  å¤§å°ç¯„åœ: {result['chunk_size_range']}")
        print(f"  å¹³å‡èªæ„ä¸€è‡´æ€§: {result['avg_coherence']:.3f}")
        print(f"  è™•ç†æ™‚é–“: {result['processing_time']:.3f}ç§’")

    # å‚³çµ±åˆ‡å‰²æ¸¬è©¦ï¼ˆæ¨¡æ“¬ï¼‰
    print("\n2. å‚³çµ±åˆ‡å‰²çµæœ:")
    legacy_results = []

    for i, article in enumerate(test_articles, 1):
        print(f"\næ–‡ç«  {i}:")

        # ç°¡å–®çš„å›ºå®šå¤§å°åˆ‡å‰²
        chunk_size = 400
        chunks = []
        start = 0

        while start < len(article):
            end = min(start + chunk_size, len(article))
            # å°‹æ‰¾å¥è™Ÿ
            if end < len(article):
                for j in range(end, max(start + chunk_size // 2, start), -1):
                    if article[j] in 'ã€‚ï¼ï¼Ÿ':
                        end = j + 1
                        break
            chunks.append(article[start:end])
            start = end

        chunk_sizes = [len(chunk) for chunk in chunks]

        result = {
            'article_length': len(article),
            'chunk_count': len(chunks),
            'avg_chunk_size': sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0,
            'chunk_size_range': f"{min(chunk_sizes)}-{max(chunk_sizes)}" if chunk_sizes else "0-0"
        }

        legacy_results.append(result)

        print(f"  åˆ‡å‰²ç‰‡æ®µæ•¸: {result['chunk_count']}")
        print(f"  å¹³å‡ç‰‡æ®µå¤§å°: {result['avg_chunk_size']:.0f} å­—ç¬¦")
        print(f"  å¤§å°ç¯„åœ: {result['chunk_size_range']}")

    # å°æ¯”åˆ†æ
    print("\n3. å°æ¯”åˆ†æ:")
    print("=" * 50)

    semantic_avg_chunks = sum(r['chunk_count'] for r in semantic_results) / len(semantic_results)
    legacy_avg_chunks = sum(r['chunk_count'] for r in legacy_results) / len(legacy_results)

    semantic_avg_size = sum(r['avg_chunk_size'] for r in semantic_results) / len(semantic_results)
    legacy_avg_size = sum(r['avg_chunk_size'] for r in legacy_results) / len(legacy_results)

    semantic_avg_coherence = sum(r['avg_coherence'] for r in semantic_results) / len(semantic_results)

    print(f"å¹³å‡åˆ‡å‰²ç‰‡æ®µæ•¸:")
    print(f"  èªæ„åˆ‡å‰²: {semantic_avg_chunks:.1f}")
    print(f"  å‚³çµ±åˆ‡å‰²: {legacy_avg_chunks:.1f}")
    print(f"  å·®ç•°: {semantic_avg_chunks - legacy_avg_chunks:+.1f}")

    print(f"\nå¹³å‡ç‰‡æ®µå¤§å°:")
    print(f"  èªæ„åˆ‡å‰²: {semantic_avg_size:.0f} å­—ç¬¦")
    print(f"  å‚³çµ±åˆ‡å‰²: {legacy_avg_size:.0f} å­—ç¬¦")
    print(f"  å·®ç•°: {semantic_avg_size - legacy_avg_size:+.0f} å­—ç¬¦")

    print(f"\nèªæ„ä¸€è‡´æ€§:")
    print(f"  èªæ„åˆ‡å‰²: {semantic_avg_coherence:.3f}")
    print(f"  å‚³çµ±åˆ‡å‰²: N/A")

    print(f"\nç¸½è™•ç†æ™‚é–“:")
    print(f"  èªæ„åˆ‡å‰²: {semantic_total_time:.3f}ç§’")
    print(f"  å‚³çµ±åˆ‡å‰²: ~0.001ç§’ (ä¼°ç®—)")

    improvement_ratio = (semantic_avg_coherence - 0.5) / 0.5 * 100 if semantic_avg_coherence > 0.5 else 0
    print(f"\né ä¼°æ”¹å–„å¹…åº¦: {improvement_ratio:.1f}%")


async def test_enhanced_vector_store():
    """æ¸¬è©¦å¢å¼·å‹å‘é‡å­˜å„²"""
    print("\n=== å¢å¼·å‹å‘é‡å­˜å„²æ¸¬è©¦ ===\n")

    try:
        # åˆå§‹åŒ–å¢å¼·å‹å‘é‡å­˜å„²
        enhanced_store = EnhancedVectorStore(
            collection_name="test_semantic_chunking",
            enable_semantic_chunking=True,
            fallback_to_legacy=True
        )

        print("å¢å¼·å‹å‘é‡å­˜å„²åˆå§‹åŒ–æˆåŠŸ")

        # æ¸¬è©¦æ–‡æª”
        test_documents = [
            "å°ç©é›»å…¬å¸ƒäº®çœ¼è²¡å ±ï¼Œç‡Ÿæ”¶å‰µæ–°é«˜ã€‚ä½†åˆ†æå¸«å°æ˜å¹´å±•æœ›ä¿å®ˆï¼Œæ“”å¿ƒæ™¯æ°£ä¸ç¢ºå®šæ€§å½±éŸ¿ã€‚",
            "è¯ç™¼ç§‘æ¨å‡ºæ–°æ™¶ç‰‡ï¼Œæ•ˆèƒ½å¤§å¹…æå‡ã€‚é è¨ˆå°‡åœ¨5Gæ‰‹æ©Ÿå¸‚å ´èˆ‡é«˜é€šæ¿€çƒˆç«¶çˆ­ã€‚"
        ]

        print(f"\næ·»åŠ  {len(test_documents)} å€‹æ¸¬è©¦æ–‡æª”...")

        # ä½¿ç”¨èªæ„åˆ‡å‰²æ·»åŠ æ–‡æª”
        result = await enhanced_store.add_documents_with_semantic_chunking(
            test_documents,
            metadatas=[{"source": "test", "category": "financial_news"} for _ in test_documents]
        )

        print("æ·»åŠ çµæœ:")
        print(f"  æ–¹æ³•: {result['method']}")
        print(f"  ç¸½æ–‡æª”æ•¸: {result['total_documents']}")
        print(f"  ç¸½åˆ‡å‰²ç‰‡æ®µæ•¸: {result['total_chunks']}")
        print(f"  å¹³å‡æ¯æ–‡æª”ç‰‡æ®µæ•¸: {result['avg_chunks_per_document']:.1f}")
        print(f"  è™•ç†æ™‚é–“: {result['processing_time']:.3f}ç§’")

        if 'avg_chunk_size' in result:
            print(f"  å¹³å‡ç‰‡æ®µå¤§å°: {result['avg_chunk_size']:.0f} å­—ç¬¦")
            print(f"  å¹³å‡èªæ„ä¸€è‡´æ€§: {result['avg_semantic_coherence']:.3f}")

        # æ¸¬è©¦æœå°‹
        print("\næ¸¬è©¦æœå°‹åŠŸèƒ½...")
        search_queries = ["å°ç©é›»è²¡å ±", "è¯ç™¼ç§‘æ™¶ç‰‡", "å¸‚å ´ç«¶çˆ­"]

        for query in search_queries:
            print(f"\næŸ¥è©¢: '{query}'")

            results = enhanced_store.search_with_metadata_filtering(
                query,
                n_results=3,
                chunking_method_filter="semantic",
                include_metadata=True
            )

            for i, result in enumerate(results, 1):
                print(f"  çµæœ {i}:")
                print(f"    ç›¸é—œæ€§: {result['relevance_score']:.3f}")
                print(f"    åˆ‡å‰²æ–¹å¼: {result.get('chunking_method', 'N/A')}")
                print(f"    èªæ„ä¸€è‡´æ€§: {result.get('semantic_coherence', 0):.3f}")
                print(f"    å…§å®¹: {result['document'][:50]}...")

        # æ€§èƒ½çµ±è¨ˆ
        print("\næ€§èƒ½çµ±è¨ˆ:")
        stats = enhanced_store.get_performance_stats()
        for key, value in stats.items():
            if key != 'configuration_summary':
                print(f"  {key}: {value}")

    except Exception as e:
        print(f"å¢å¼·å‹å‘é‡å­˜å„²æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()


async def test_configuration_system():
    """æ¸¬è©¦é…ç½®ç³»çµ±"""
    print("\n=== é…ç½®ç³»çµ±æ¸¬è©¦ ===\n")

    # å‰µå»ºé è¨­é…ç½®æª”æ¡ˆ
    config_file = "test_semantic_chunking.yaml"
    create_default_config_file(config_file)
    print(f"å‰µå»ºé è¨­é…ç½®æª”æ¡ˆ: {config_file}")

    # æ¸¬è©¦é…ç½®è¼‰å…¥
    config_manager = ConfigManager(config_file)
    config_summary = config_manager.get_summary()

    print("\nç•¶å‰é…ç½®æ‘˜è¦:")
    for key, value in config_summary.items():
        print(f"  {key}: {value}")

    # æ¸¬è©¦å‹•æ…‹é…ç½®æ›´æ–°
    print("\næ¸¬è©¦å‹•æ…‹é…ç½®æ›´æ–°...")
    config_manager.update_config({
        'chunk_size': {
            'target_size': 600
        },
        'boundary': {
            'similarity_threshold': 0.8
        }
    })

    updated_summary = config_manager.get_summary()
    print("æ›´æ–°å¾Œé…ç½®:")
    print(f"  target_chunk_size: {updated_summary['target_chunk_size']}")
    print(f"  similarity_threshold: {updated_summary['similarity_threshold']}")

    # æ¸…ç†æ¸¬è©¦æª”æ¡ˆ
    Path(config_file).unlink(missing_ok=True)
    print(f"\næ¸…ç†æ¸¬è©¦æª”æ¡ˆ: {config_file}")


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸ”¬ èªæ„åˆ‡å‰²ç³»çµ±å®Œæ•´æ¸¬è©¦\n")
    print("=" * 60)

    test_functions = [
        ("åŸºç¤èªæ„åˆ‡å‰²", test_semantic_chunking_basic),
        ("å°æ¯”æ¸¬è©¦", test_vs_legacy_chunking),
        ("å¢å¼·å‹å‘é‡å­˜å„²", test_enhanced_vector_store),
        ("é…ç½®ç³»çµ±", test_configuration_system)
    ]

    for test_name, test_func in test_functions:
        print(f"\nğŸ§ª åŸ·è¡Œæ¸¬è©¦: {test_name}")
        print("-" * 40)

        try:
            start_time = time.time()
            await test_func()
            test_time = time.time() - start_time
            print(f"\nâœ… {test_name} æ¸¬è©¦å®Œæˆ (è€—æ™‚: {test_time:.2f}ç§’)")

        except Exception as e:
            print(f"\nâŒ {test_name} æ¸¬è©¦å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()

        print("\n" + "=" * 60)

    print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦åŸ·è¡Œå®Œæˆï¼")

    # æä¾›ä½¿ç”¨å»ºè­°
    print("\nğŸ“ ä½¿ç”¨å»ºè­°:")
    print("1. æ ¹æ“šæ¸¬è©¦çµæœèª¿æ•´é…ç½®åƒæ•¸")
    print("2. åœ¨ç”Ÿç”¢ç’°å¢ƒä½¿ç”¨å‰ï¼Œå»ºè­°é€²è¡Œæ›´å¤§è¦æ¨¡çš„ A/B æ¸¬è©¦")
    print("3. ç›£æ§èªæ„ä¸€è‡´æ€§æŒ‡æ¨™ï¼Œç¢ºä¿åˆ‡å‰²å“è³ª")
    print("4. å®šæœŸæª¢æŸ¥å’Œæ›´æ–°è²¡ç¶“é—œéµè©åˆ—è¡¨")


if __name__ == "__main__":
    asyncio.run(main())