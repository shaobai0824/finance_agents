#!/usr/bin/env python3
"""
LLM é…ç½®æª¢æŸ¥å’Œè¨­å®šå·¥å…·
"""

import os
import sys
import asyncio
from pathlib import Path

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(str(Path(__file__).parent / 'src' / 'main' / 'python'))

from llm import llm_manager, is_llm_configured, generate_llm_response


def check_environment_variables():
    """æª¢æŸ¥ç’°å¢ƒè®Šæ•¸é…ç½®"""
    print("[æª¢æŸ¥] LLM ç’°å¢ƒè®Šæ•¸é…ç½®...")
    print("=" * 50)

    # OpenAI é…ç½®
    openai_key = os.getenv("OPENAI_API_KEY")
    print(f"OPENAI_API_KEY: {'[å·²è¨­å®š]' if openai_key else '[æœªè¨­å®š]'}")
    if openai_key:
        print(f"  é‡‘é‘°é•·åº¦: {len(openai_key)} å­—å…ƒ")
        print(f"  å‰ç¶´: {openai_key[:7]}...")

    # Anthropic é…ç½®
    anthropic_key = os.getenv("ANTHROPIC_API_KEY")
    print(f"ANTHROPIC_API_KEY: {'âœ… å·²è¨­å®š' if anthropic_key else 'âŒ æœªè¨­å®š'}")
    if anthropic_key:
        print(f"  é‡‘é‘°é•·åº¦: {len(anthropic_key)} å­—å…ƒ")
        print(f"  å‰ç¶´: {anthropic_key[:7]}...")

    print()


def check_available_clients():
    """æª¢æŸ¥å¯ç”¨çš„ LLM å®¢æˆ¶ç«¯"""
    print("ğŸ¤– æª¢æŸ¥å¯ç”¨çš„ LLM å®¢æˆ¶ç«¯...")
    print("=" * 50)

    available_clients = llm_manager.get_available_clients()
    print(f"å¯ç”¨å®¢æˆ¶ç«¯: {', '.join(available_clients)}")
    print(f"é è¨­å®¢æˆ¶ç«¯: {llm_manager.default_client}")
    print(f"çœŸå¯¦ LLM å¯ç”¨: {'âœ… æ˜¯' if is_llm_configured() else 'âŒ å¦'}")
    print()


async def test_llm_response():
    """æ¸¬è©¦ LLM å›æ‡‰"""
    print("ğŸ§ª æ¸¬è©¦ LLM å›æ‡‰...")
    print("=" * 50)

    test_prompt = "è«‹ç°¡å–®ä»‹ç´¹ä»€éº¼æ˜¯æŠ•è³‡é¢¨éšªï¼Ÿ"

    try:
        print(f"æ¸¬è©¦æç¤º: {test_prompt}")
        print("ç”Ÿæˆå›æ‡‰ä¸­...")

        response = await generate_llm_response(test_prompt, max_tokens=200)

        print(f"âœ… å›æ‡‰æˆåŠŸï¼")
        print(f"æ¨¡å‹: {response.model}")
        print(f"å›æ‡‰æ™‚é–“: {response.response_time:.2f} ç§’")
        print(f"å›æ‡‰å…§å®¹:")
        print("-" * 30)
        print(response.content)
        print("-" * 30)

    except Exception as e:
        print(f"âŒ å›æ‡‰å¤±æ•—: {e}")

    print()


def show_configuration_guide():
    """é¡¯ç¤ºé…ç½®æŒ‡å—"""
    print("ğŸ“– LLM é…ç½®æŒ‡å—")
    print("=" * 50)

    print("1. è¨­å®š OpenAI API é‡‘é‘°:")
    print("   export OPENAI_API_KEY=your-openai-api-key-here")
    print("   æˆ–åœ¨ .env æª”æ¡ˆä¸­æ·»åŠ : OPENAI_API_KEY=your-openai-api-key-here")
    print()

    print("2. è¨­å®š Anthropic API é‡‘é‘°:")
    print("   export ANTHROPIC_API_KEY=your-anthropic-api-key-here")
    print("   æˆ–åœ¨ .env æª”æ¡ˆä¸­æ·»åŠ : ANTHROPIC_API_KEY=your-anthropic-api-key-here")
    print()

    print("3. å®‰è£å¿…è¦å¥—ä»¶:")
    print("   pip install openai anthropic")
    print()

    print("4. å¦‚ä½•å–å¾— API é‡‘é‘°:")
    print("   - OpenAI: https://platform.openai.com/api-keys")
    print("   - Anthropic: https://console.anthropic.com/")
    print()


def show_usage_example():
    """é¡¯ç¤ºä½¿ç”¨ç¯„ä¾‹"""
    print("ğŸ’¡ ä½¿ç”¨ç¯„ä¾‹")
    print("=" * 50)

    example_code = '''
# åœ¨æ‚¨çš„ agent ä¸­ä½¿ç”¨ LLM
from llm import generate_llm_response, is_llm_configured

async def generate_analysis(query: str):
    if is_llm_configured():
        response = await generate_llm_response(
            prompt=f"è«‹åˆ†æä»¥ä¸‹æŠ•è³‡å•é¡Œ: {query}",
            max_tokens=500,
            temperature=0.3
        )
        return response.content
    else:
        return "LLM æœªé…ç½®ï¼Œä½¿ç”¨é è¨­å›æ‡‰"
'''

    print(example_code)


async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ è²¡ç¶“ç†è²¡ç³»çµ± LLM é…ç½®æª¢æŸ¥")
    print("=" * 50)
    print()

    # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
    check_environment_variables()

    # æª¢æŸ¥å¯ç”¨å®¢æˆ¶ç«¯
    check_available_clients()

    # æ¸¬è©¦ LLM å›æ‡‰
    await test_llm_response()

    # å¦‚æœæ²’æœ‰é…ç½®çœŸå¯¦ LLMï¼Œé¡¯ç¤ºé…ç½®æŒ‡å—
    if not is_llm_configured():
        print("âš ï¸ æœªæª¢æ¸¬åˆ°çœŸå¯¦ LLM é…ç½®ï¼Œç›®å‰ä½¿ç”¨æ¨¡æ“¬å›æ‡‰")
        print()
        show_configuration_guide()
        show_usage_example()
    else:
        print("ğŸ‰ LLM é…ç½®å®Œæˆï¼æ‚¨çš„ç³»çµ±å·²å¯ä½¿ç”¨çœŸå¯¦çš„ AI å›æ‡‰ã€‚")
        print()
        print("ç¾åœ¨æ‚¨å¯ä»¥:")
        print("- ä½¿ç”¨ FinancialAnalystAgentLLM ç²å¾—çœŸå¯¦çš„ AI åˆ†æ")
        print("- é«”é©—æ›´æ™ºèƒ½å’Œå€‹äººåŒ–çš„ç†è²¡å»ºè­°")
        print("- äº«å—åŸºæ–¼ LLM çš„è‡ªç„¶èªè¨€å°è©±")


if __name__ == "__main__":
    # å˜—è©¦è¼‰å…¥ .env æª”æ¡ˆ
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("âœ… å·²è¼‰å…¥ .env æª”æ¡ˆ")
    except ImportError:
        print("âš ï¸ python-dotenv æœªå®‰è£ï¼Œè·³é .env æª”æ¡ˆè¼‰å…¥")
    except Exception:
        print("âš ï¸ ç„¡æ³•è¼‰å…¥ .env æª”æ¡ˆ")

    print()
    asyncio.run(main())