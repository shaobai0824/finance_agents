#!/usr/bin/env python3
"""
è³‡æ–™è¼‰å…¥å™¨ - å°‡ç†è²¡çŸ¥è­˜åŒ¯å…¥å‘é‡è³‡æ–™åº«

æ”¯æ´ï¼š
1. æ–‡å­—æª”æ¡ˆæ‰¹æ¬¡åŒ¯å…¥
2. ç¯„ä¾‹ç†è²¡çŸ¥è­˜å»ºç«‹
3. å‘é‡åŒ–å’Œå­˜å„²
"""

import sys
import os
import asyncio
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.main.python.rag import ChromaVectorStore

class DataLoader:
    """è³‡æ–™è¼‰å…¥å™¨"""

    def __init__(self):
        self.vector_store = ChromaVectorStore()

    def create_sample_data(self) -> List[Dict[str, Any]]:
        """å»ºç«‹ç¯„ä¾‹ç†è²¡çŸ¥è­˜è³‡æ–™"""

        sample_documents = [
            # æŠ•è³‡ç†è²¡è¦åŠƒé¡
            {
                "content": """
å¹´è¼•äººæŠ•è³‡å»ºè­°ï¼ˆ20-30æ­²ï¼‰ï¼š
1. æŠ•è³‡æœŸé–“é•·ï¼Œå¯æ‰¿å—è¼ƒé«˜é¢¨éšª
2. å»ºè­°è‚¡ç¥¨å æ¯” 70-80%ï¼Œå‚µåˆ¸ 20-30%
3. å®šæœŸå®šé¡æŠ•è³‡ï¼ŒåŸ¹é¤ŠæŠ•è³‡ç¿’æ…£
4. å„ªå…ˆæŠ•è³‡æŒ‡æ•¸å‹åŸºé‡‘ï¼ˆETFï¼‰åˆ†æ•£é¢¨éšª
5. å»ºç«‹ç·Šæ€¥å‚™ç”¨é‡‘ï¼ˆ3-6å€‹æœˆç”Ÿæ´»è²»ï¼‰
6. å–„ç”¨è¤‡åˆ©æ•ˆæ‡‰ï¼Œè¶Šæ—©é–‹å§‹è¶Šå¥½
                """,
                "metadata": {
                    "source": "ç†è²¡è¦åŠƒæ‰‹å†Š",
                    "domain": "financial_planning",
                    "target_age": "20-30",
                    "risk_level": "moderate_to_high"
                }
            },
            {
                "content": """
ä¸­å¹´æŠ•è³‡ç­–ç•¥ï¼ˆ40-50æ­²ï¼‰ï¼š
1. æŠ•è³‡æœŸé–“ç¸®çŸ­ï¼Œé™ä½é¢¨éšªæ‰¿å—åº¦
2. å»ºè­°è‚¡ç¥¨å æ¯” 50-60%ï¼Œå‚µåˆ¸ 40-50%
3. å¢åŠ ä¿éšªè¦åŠƒï¼Œä¿éšœå®¶åº­æ”¶å…¥
4. è€ƒæ…®å­å¥³æ•™è‚²åŸºé‡‘æº–å‚™
5. é–‹å§‹è¦åŠƒé€€ä¼‘é‡‘ç´¯ç©
6. åˆ†æ•£æŠ•è³‡æ–¼ä¸åŒåœ°å€å’Œç”¢æ¥­
                """,
                "metadata": {
                    "source": "ç†è²¡è¦åŠƒæ‰‹å†Š",
                    "domain": "financial_planning",
                    "target_age": "40-50",
                    "risk_level": "moderate"
                }
            },
            {
                "content": """
é€€ä¼‘æŠ•è³‡è¦åŠƒï¼ˆ50æ­²ä»¥ä¸Šï¼‰ï¼š
1. ä»¥ä¿æœ¬ç‚ºä¸»è¦ç›®æ¨™
2. å»ºè­°è‚¡ç¥¨å æ¯” 30-40%ï¼Œå‚µåˆ¸ 60-70%
3. å¢åŠ ç¾é‡‘éƒ¨ä½ï¼Œæé«˜æµå‹•æ€§
4. è€ƒæ…®å¹´é‡‘ä¿éšªç­‰ä¿è­‰æ”¶ç›Šå•†å“
5. é™ä½æŠ•è³‡é¢¨éšªï¼Œé¿å…å¤§å¹…è™§æ
6. è¦åŠƒé€€ä¼‘å¾Œçš„ç¾é‡‘æµéœ€æ±‚
                """,
                "metadata": {
                    "source": "ç†è²¡è¦åŠƒæ‰‹å†Š",
                    "domain": "financial_planning",
                    "target_age": "50+",
                    "risk_level": "conservative"
                }
            },

            # å¸‚å ´åˆ†æé¡
            {
                "content": """
è‚¡ç¥¨æŠ•è³‡åŸºæœ¬é¢åˆ†æï¼š
1. å…¬å¸è²¡å‹™å ±è¡¨åˆ†æï¼ˆè³‡ç”¢è² å‚µè¡¨ã€æç›Šè¡¨ã€ç¾é‡‘æµé‡è¡¨ï¼‰
2. è©•ä¼°æŒ‡æ¨™ï¼šæœ¬ç›Šæ¯”ï¼ˆP/Eï¼‰ã€è‚¡åƒ¹æ·¨å€¼æ¯”ï¼ˆP/Bï¼‰ã€è‚¡æ±æ¬Šç›Šå ±é…¬ç‡ï¼ˆROEï¼‰
3. ç”¢æ¥­ç«¶çˆ­åŠ›å’Œå¸‚å ´åœ°ä½åˆ†æ
4. ç®¡ç†éšå±¤ç´ è³ªå’Œå…¬å¸æ²»ç†
5. é•·æœŸæˆé•·æ½›åŠ›å’Œç‡Ÿé‹æ¨¡å¼
6. åˆ†æå¸«è©•ç­‰å’Œç›®æ¨™åƒ¹åƒè€ƒ
                """,
                "metadata": {
                    "source": "æŠ•è³‡åˆ†ææŒ‡å—",
                    "domain": "financial_analysis",
                    "investment_type": "stocks",
                    "analysis_method": "fundamental"
                }
            },
            {
                "content": """
å‚µåˆ¸æŠ•è³‡åˆ†æï¼š
1. ä¿¡ç”¨è©•ç­‰ï¼šæ”¿åºœå‚µåˆ¸ > æŠ•è³‡ç´šå…¬å¸å‚µ > é«˜æ”¶ç›Šå‚µåˆ¸
2. åˆ©ç‡é¢¨éšªï¼šå‚µåˆ¸åƒ¹æ ¼èˆ‡åˆ©ç‡å‘ˆåå‘é—œä¿‚
3. å­˜çºŒæœŸé–“ï¼šè¼ƒé•·æœŸå‚µåˆ¸åˆ©ç‡æ•æ„Ÿåº¦è¼ƒé«˜
4. åˆ°æœŸæ”¶ç›Šç‡ï¼ˆYTMï¼‰è¨ˆç®—å’Œæ¯”è¼ƒ
5. é€šè†¨ä¿è­·å‚µåˆ¸ï¼ˆTIPSï¼‰è€ƒé‡
6. æŠ•è³‡ç´šå‚µåˆ¸ä½œç‚ºè³‡ç”¢é…ç½®åŸºçŸ³
                """,
                "metadata": {
                    "source": "æŠ•è³‡åˆ†ææŒ‡å—",
                    "domain": "financial_analysis",
                    "investment_type": "bonds",
                    "analysis_method": "fundamental"
                }
            },

            # æ³•è¦åˆè¦é¡
            {
                "content": """
å°ç£å€‹äººæ‰€å¾—ç¨…æŠ•è³‡ç›¸é—œè¦å®šï¼š
1. è‚¡ç¥¨äº¤æ˜“æ‰€å¾—ç›®å‰å…ç¨…
2. å‚µåˆ¸åˆ©æ¯æ‰€å¾—éœ€ç”³å ±ç¶œåˆæ‰€å¾—ç¨…
3. åŸºé‡‘é…æ¯è¶…é27è¬éœ€ç”³å ±
4. æµ·å¤–æŠ•è³‡æ‰€å¾—è¶…é100è¬éœ€ç”³å ±
5. æœ€ä½ç¨…è² åˆ¶ï¼šæµ·å¤–æ‰€å¾— + è­‰åˆ¸äº¤æ˜“æ‰€å¾—ç­‰ç‰¹å®šé …ç›®
6. æŠ•è³‡æ‰£é™¤é¡ï¼šæ¯å¹´27è¬å…ƒå„²è“„æŠ•è³‡ç‰¹åˆ¥æ‰£é™¤é¡
                """,
                "metadata": {
                    "source": "ç¨…å‹™æ³•è¦æ‰‹å†Š",
                    "domain": "legal_compliance",
                    "regulation_type": "tax",
                    "jurisdiction": "taiwan"
                }
            },
            {
                "content": """
é‡‘èæ¶ˆè²»è€…ä¿è­·é‡é»ï¼š
1. æŠ•è³‡å‰å……åˆ†äº†è§£å•†å“é¢¨éšª
2. è©³é–±æŠ•è³‡èªªæ˜æ›¸å’Œé‡è¦è³‡è¨Š
3. ç¢ºèªæ¥­è€…åˆæ³•æ€§å’Œé‡‘ç®¡æœƒè¨±å¯
4. ä¿ç•™æ‰€æœ‰äº¤æ˜“ç´€éŒ„å’Œç›¸é—œæ–‡ä»¶
5. å¦‚æœ‰çˆ­è­°å¯å‘é‡‘èæ¶ˆè²»è©•è­°ä¸­å¿ƒç”³è¨´
6. æ³¨æ„å†·éœæœŸè¦å®šå’Œè§£ç´„æ¬Šåˆ©
                """,
                "metadata": {
                    "source": "é‡‘èæ³•è¦æŒ‡å—",
                    "domain": "legal_compliance",
                    "regulation_type": "consumer_protection",
                    "jurisdiction": "taiwan"
                }
            },

            # é¢¨éšªç®¡ç†é¡
            {
                "content": """
æŠ•è³‡é¢¨éšªç®¡ç†åŸå‰‡ï¼š
1. åˆ†æ•£æŠ•è³‡ï¼šä¸è¦æŠŠé›è›‹æ”¾åœ¨åŒä¸€å€‹ç±ƒå­è£¡
2. è³‡ç”¢é…ç½®ï¼šè‚¡ç¥¨ã€å‚µåˆ¸ã€ç¾é‡‘çš„é©ç•¶æ¯”ä¾‹
3. å®šæœŸæª¢è¦–ï¼šè‡³å°‘æ¯å­£æª¢è¨æŠ•è³‡çµ„åˆ
4. é¢¨éšªæ‰¿å—åº¦è©•ä¼°ï¼šå¹´é½¡ã€æ”¶å…¥ã€æŠ•è³‡ç¶“é©—
5. åœææ©Ÿåˆ¶ï¼šè¨­å®šå¯æ¥å—çš„æœ€å¤§è™§æ
6. æƒ…ç·’ç®¡ç†ï¼šé¿å…è¿½é«˜æ®ºä½çš„æƒ…ç·’æ€§æ“ä½œ
                """,
                "metadata": {
                    "source": "é¢¨éšªç®¡ç†æ‰‹å†Š",
                    "domain": "financial_planning",
                    "topic": "risk_management",
                    "level": "basic"
                }
            },

            # ETF æŠ•è³‡é¡
            {
                "content": """
ETFï¼ˆæŒ‡æ•¸è‚¡ç¥¨å‹åŸºé‡‘ï¼‰æŠ•è³‡å„ªå‹¢ï¼š
1. åˆ†æ•£é¢¨éšªï¼šä¸€æ¬¡æŠ•è³‡å¤šæª”è‚¡ç¥¨
2. æˆæœ¬ä½å»‰ï¼šç®¡ç†è²»é€šå¸¸ä½æ–¼ä¸»å‹•å¼åŸºé‡‘
3. é€æ˜åº¦é«˜ï¼šæ¯æ—¥å…¬å¸ƒæŒè‚¡æ˜ç´°
4. æµå‹•æ€§ä½³ï¼šå¯åœ¨è‚¡å¸‚äº¤æ˜“æ™‚é–“è²·è³£
5. ç¨…å‹™æ•ˆç‡ï¼šè¢«å‹•è¿½è¹¤æŒ‡æ•¸ï¼Œè¼ƒå°‘é…æ¯
6. é©åˆé•·æœŸæŠ•è³‡å’Œå®šæœŸå®šé¡
                """,
                "metadata": {
                    "source": "ETFæŠ•è³‡æŒ‡å—",
                    "domain": "financial_planning",
                    "investment_type": "etf",
                    "strategy": "passive_investing"
                }
            }
        ]

        return sample_documents

    async def load_sample_data(self):
        """è¼‰å…¥ç¯„ä¾‹è³‡æ–™åˆ°å‘é‡è³‡æ–™åº«"""
        print("ğŸ”„ è¼‰å…¥ç¯„ä¾‹ç†è²¡çŸ¥è­˜åˆ°å‘é‡è³‡æ–™åº«...")

        sample_docs = self.create_sample_data()

        # æº–å‚™æ–‡ä»¶å’Œå…ƒè³‡æ–™
        documents = [doc["content"].strip() for doc in sample_docs]
        metadatas = [doc["metadata"] for doc in sample_docs]

        try:
            # æ‰¹æ¬¡æ–°å¢æ–‡ä»¶
            doc_ids = self.vector_store.add_documents(
                documents=documents,
                metadatas=metadatas
            )

            print(f"âœ… æˆåŠŸè¼‰å…¥ {len(doc_ids)} ç­†ç†è²¡çŸ¥è­˜")
            print(f"ğŸ“„ æ–‡ä»¶ IDs: {doc_ids[:3]}...")  # é¡¯ç¤ºå‰3å€‹ID

            # é¡¯ç¤ºè³‡æ–™åº«ç‹€æ…‹
            info = self.vector_store.get_collection_info()
            print(f"ğŸ“Š è³‡æ–™åº«ç‹€æ…‹: {info['document_count']} ç­†æ–‡ä»¶")

        except Exception as e:
            print(f"âŒ è¼‰å…¥å¤±æ•—: {e}")
            raise

    async def test_retrieval(self):
        """æ¸¬è©¦çŸ¥è­˜æª¢ç´¢åŠŸèƒ½"""
        print("\nğŸ” æ¸¬è©¦çŸ¥è­˜æª¢ç´¢åŠŸèƒ½...")

        test_queries = [
            "30æ­²æŠ•è³‡å»ºè­°",
            "è‚¡ç¥¨åˆ†ææ–¹æ³•",
            "å°ç£ç¨…å‹™è¦å®š",
            "ETFæŠ•è³‡å„ªå‹¢"
        ]

        for query in test_queries:
            print(f"\næŸ¥è©¢: {query}")
            results = self.vector_store.search(query, n_results=2)

            for i, result in enumerate(results, 1):
                similarity = 1 - result['distance']  # è½‰æ›ç‚ºç›¸ä¼¼åº¦
                print(f"  {i}. ç›¸ä¼¼åº¦: {similarity:.2%}")
                print(f"     å…§å®¹: {result['document'][:100]}...")
                print(f"     é ˜åŸŸ: {result['metadata'].get('domain', 'unknown')}")

    async def load_from_json(self, json_file_path: str):
        """å¾ JSON æª”æ¡ˆè¼‰å…¥çµæ§‹åŒ–è³‡æ–™"""
        import json

        print(f"ğŸ“¥ å¾ JSON æª”æ¡ˆè¼‰å…¥: {json_file_path}")

        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            if not isinstance(data, list):
                raise ValueError("JSON æª”æ¡ˆå¿…é ˆæ˜¯åŒ…å«æ–‡ä»¶ç‰©ä»¶çš„é™£åˆ—")

            documents = []
            metadatas = []

            for item in data:
                if 'content' not in item:
                    print(f"âš ï¸  è·³éç¼ºå°‘ 'content' çš„é …ç›®: {item}")
                    continue

                documents.append(item['content'])
                metadatas.append(item.get('metadata', {}))

            if documents:
                doc_ids = self.vector_store.add_documents(
                    documents=documents,
                    metadatas=metadatas
                )
                print(f"âœ… æˆåŠŸè¼‰å…¥ {len(doc_ids)} ç­†è³‡æ–™å¾ JSON")
                return doc_ids
            else:
                print("âŒ JSON æª”æ¡ˆä¸­æ²’æœ‰æœ‰æ•ˆçš„æ–‡ä»¶")
                return []

        except Exception as e:
            print(f"âŒ JSON è¼‰å…¥å¤±æ•—: {e}")
            raise

    async def load_from_txt(self, txt_file_path: str, chunk_size: int = 500):
        """å¾ TXT æª”æ¡ˆè¼‰å…¥ç´”æ–‡å­—è³‡æ–™"""
        print(f"ğŸ“¥ å¾ TXT æª”æ¡ˆè¼‰å…¥: {txt_file_path}")

        try:
            with open(txt_file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # æŒ‰æ®µè½åˆ†å‰²ï¼ˆé›™æ›è¡Œï¼‰
            sections = content.split('\n---\n')

            documents = []
            metadatas = []

            for i, section in enumerate(sections):
                section = section.strip()
                if not section:
                    continue

                # æå–æ¨™é¡Œï¼ˆå¦‚æœæœ‰ ## é–‹é ­ï¼‰
                lines = section.split('\n')
                title = f"Section {i+1}"
                for line in lines:
                    if line.startswith('##'):
                        title = line.replace('##', '').strip()
                        break

                # å¦‚æœå…§å®¹å¤ªé•·ï¼Œåˆ†å‰²æˆè¼ƒå°çš„å¡Š
                if len(section) > chunk_size:
                    chunks = [section[i:i+chunk_size] for i in range(0, len(section), chunk_size)]
                    for j, chunk in enumerate(chunks):
                        documents.append(chunk)
                        metadatas.append({
                            "source": txt_file_path,
                            "section": title,
                            "chunk": j+1,
                            "domain": "general"
                        })
                else:
                    documents.append(section)
                    metadatas.append({
                        "source": txt_file_path,
                        "section": title,
                        "domain": "general"
                    })

            if documents:
                doc_ids = self.vector_store.add_documents(
                    documents=documents,
                    metadatas=metadatas
                )
                print(f"âœ… æˆåŠŸè¼‰å…¥ {len(doc_ids)} ç­†è³‡æ–™å¾ TXT")
                return doc_ids
            else:
                print("âŒ TXT æª”æ¡ˆä¸­æ²’æœ‰æœ‰æ•ˆçš„å…§å®¹")
                return []

        except Exception as e:
            print(f"âŒ TXT è¼‰å…¥å¤±æ•—: {e}")
            raise

    async def load_from_directory(self, directory_path: str):
        """å¾ç›®éŒ„è¼‰å…¥æ‰€æœ‰æ”¯æ´çš„æª”æ¡ˆ"""
        directory = Path(directory_path)
        if not directory.exists():
            print(f"âŒ ç›®éŒ„ä¸å­˜åœ¨: {directory_path}")
            return

        print(f"ğŸ“ æƒæç›®éŒ„: {directory_path}")

        total_loaded = 0

        # è¼‰å…¥ JSON æª”æ¡ˆ
        for json_file in directory.glob("*.json"):
            try:
                doc_ids = await self.load_from_json(str(json_file))
                total_loaded += len(doc_ids)
            except Exception as e:
                print(f"âš ï¸  è·³éæª”æ¡ˆ {json_file}: {e}")

        # è¼‰å…¥ TXT æª”æ¡ˆ
        for txt_file in directory.glob("*.txt"):
            try:
                doc_ids = await self.load_from_txt(str(txt_file))
                total_loaded += len(doc_ids)
            except Exception as e:
                print(f"âš ï¸  è·³éæª”æ¡ˆ {txt_file}: {e}")

        print(f"ğŸ‰ ç›®éŒ„è¼‰å…¥å®Œæˆï¼Œç¸½è¨ˆè¼‰å…¥ {total_loaded} ç­†è³‡æ–™")

    def clear_database(self):
        """æ¸…ç©ºè³‡æ–™åº«ï¼ˆè¬¹æ…ä½¿ç”¨ï¼‰"""
        print("âš ï¸  æ¸…ç©ºå‘é‡è³‡æ–™åº«...")
        self.vector_store.clear_collection()
        print("âœ… è³‡æ–™åº«å·²æ¸…ç©º")

async def main():
    """ä¸»å‡½å¼"""
    print("ğŸ¦ Finance Agents è³‡æ–™è¼‰å…¥å™¨")
    print("=" * 40)

    loader = DataLoader()

    # æª¢æŸ¥ç•¶å‰è³‡æ–™åº«ç‹€æ…‹
    info = loader.vector_store.get_collection_info()
    print(f"ğŸ“Š ç•¶å‰è³‡æ–™åº«: {info['document_count']} ç­†æ–‡ä»¶")

    print("\né¸æ“‡è¼‰å…¥æ–¹å¼:")
    print("1. è¼‰å…¥å…§å»ºç¯„ä¾‹è³‡æ–™")
    print("2. å¾ JSON æª”æ¡ˆè¼‰å…¥")
    print("3. å¾ TXT æª”æ¡ˆè¼‰å…¥")
    print("4. å¾ data/ ç›®éŒ„è¼‰å…¥æ‰€æœ‰æª”æ¡ˆ")
    print("5. æ¸…ç©ºè³‡æ–™åº«")
    print("6. è·³éè¼‰å…¥ï¼Œç›´æ¥æ¸¬è©¦")

    choice = input("\nè«‹é¸æ“‡ (1-6): ").strip()

    if choice == "1":
        if info['document_count'] > 0:
            response = input("è³‡æ–™åº«å·²æœ‰è³‡æ–™ï¼Œæ˜¯å¦è¦æ¸…ç©ºä¸¦é‡æ–°è¼‰å…¥ï¼Ÿ(y/N): ")
            if response.lower() == 'y':
                loader.clear_database()
        await loader.load_sample_data()

    elif choice == "2":
        json_path = input("è«‹è¼¸å…¥ JSON æª”æ¡ˆè·¯å¾‘ (é è¨­: data/sample_knowledge.json): ").strip()
        if not json_path:
            json_path = "data/sample_knowledge.json"
        try:
            await loader.load_from_json(json_path)
        except Exception as e:
            print(f"è¼‰å…¥å¤±æ•—: {e}")

    elif choice == "3":
        txt_path = input("è«‹è¼¸å…¥ TXT æª”æ¡ˆè·¯å¾‘ (é è¨­: data/sample_knowledge.txt): ").strip()
        if not txt_path:
            txt_path = "data/sample_knowledge.txt"
        try:
            await loader.load_from_txt(txt_path)
        except Exception as e:
            print(f"è¼‰å…¥å¤±æ•—: {e}")

    elif choice == "4":
        data_dir = input("è«‹è¼¸å…¥ç›®éŒ„è·¯å¾‘ (é è¨­: data/): ").strip()
        if not data_dir:
            data_dir = "data/"
        try:
            await loader.load_from_directory(data_dir)
        except Exception as e:
            print(f"è¼‰å…¥å¤±æ•—: {e}")

    elif choice == "5":
        confirm = input("ç¢ºå®šè¦æ¸…ç©ºè³‡æ–™åº«ï¼Ÿæ­¤æ“ä½œç„¡æ³•å¾©åŸ (y/N): ")
        if confirm.lower() == 'y':
            loader.clear_database()
        else:
            print("å–æ¶ˆæ¸…ç©ºæ“ä½œ")

    elif choice == "6":
        print("â­ï¸  è·³éè³‡æ–™è¼‰å…¥")

    else:
        print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè·³éè¼‰å…¥")

    # é¡¯ç¤ºæœ€æ–°è³‡æ–™åº«ç‹€æ…‹
    info = loader.vector_store.get_collection_info()
    print(f"\nğŸ“Š æœ€çµ‚è³‡æ–™åº«ç‹€æ…‹: {info['document_count']} ç­†æ–‡ä»¶")

    # æ¸¬è©¦æª¢ç´¢åŠŸèƒ½
    if info['document_count'] > 0:
        test_choice = input("\næ˜¯å¦è¦æ¸¬è©¦æª¢ç´¢åŠŸèƒ½ï¼Ÿ(Y/n): ").strip()
        if test_choice.lower() != 'n':
            await loader.test_retrieval()

    print("\nğŸ‰ è³‡æ–™è¼‰å…¥å™¨åŸ·è¡Œå®Œæˆï¼")
    print("ğŸ’¡ å»ºè­°åŸ·è¡Œ: python simple_test.py")
    print("ğŸŒ æˆ–é–‹å•Ÿ: test_frontend.html")

if __name__ == "__main__":
    asyncio.run(main())