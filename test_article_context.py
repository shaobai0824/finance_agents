#!/usr/bin/env python3
"""
æ–‡ç« é—œè¯æª¢ç´¢æ¸¬è©¦å·¥å…·

é©—è­‰åŒç¯‡æ–‡ç« çš„ chunks æ˜¯å¦èƒ½è¢«é—œè¯æª¢ç´¢
æ¸¬è©¦æ–‡ç« ä¸Šä¸‹æ–‡å®Œæ•´æ€§
"""

import asyncio
import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src" / "main" / "python"))

from dotenv import load_dotenv
load_dotenv()

from src.main.python.rag.enhanced_vector_store import EnhancedVectorStore


async def test_article_context_retrieval():
    """æ¸¬è©¦æ–‡ç« ä¸Šä¸‹æ–‡æª¢ç´¢åŠŸèƒ½"""
    print("=== æ–‡ç« ä¸Šä¸‹æ–‡æª¢ç´¢æ¸¬è©¦ ===\n")

    # åˆå§‹åŒ–å¢å¼·å‹å‘é‡å­˜å„²
    store = EnhancedVectorStore(
        collection_name="test_article_context",
        enable_semantic_chunking=True,
        fallback_to_legacy=True
    )

    # å‰µå»ºæ¸¬è©¦æ–‡ç« ï¼ˆæ¨¡æ“¬è¢«åˆ‡å‰²çš„è²¡ç¶“æ–°èï¼‰
    test_article = """
    å°ç©é›»ä»Šæ—¥å…¬å¸ƒç¬¬ä¸‰å­£è²¡å ±ï¼Œè¡¨ç¾è¶…è¶Šå¸‚å ´é æœŸã€‚
    ç‡Ÿæ”¶é”åˆ°æ–°å°å¹£6,136å„„å…ƒï¼Œè¼ƒå‰å­£æˆé•·14.6%ï¼Œè¼ƒå»å¹´åŒæœŸæˆé•·16.9%ã€‚
    æ¯›åˆ©ç‡ç‚º54.3%ï¼Œç‡Ÿæ¥­åˆ©ç›Šç‡ç‚º45.1%ã€‚

    åŸ·è¡Œé•·é­å“²å®¶åœ¨æ³•èªªæœƒä¸­è¡¨ç¤ºï¼Œå…¬å¸å—æƒ æ–¼æ™ºæ…§å‹æ‰‹æ©Ÿå’Œé«˜æ•ˆèƒ½é‹ç®—éœ€æ±‚å¼·å‹ã€‚
    ç‰¹åˆ¥æ˜¯è˜‹æœiPhone 15ç³»åˆ—çš„å°å…¥ï¼Œç‚ºå…¬å¸å¸¶ä¾†é¡¯è‘—çš„ç‡Ÿæ”¶è²¢ç»ã€‚
    é è¨ˆç¬¬å››å­£ç‡Ÿæ”¶å°‡æŒçºŒæˆé•·ï¼Œä½†æˆé•·å¹…åº¦å¯èƒ½è¶¨ç·©ã€‚

    ç„¶è€Œï¼Œæ³•äººåˆ†æå¸«å°æ–¼æ˜å¹´ç¬¬ä¸€å­£çš„å±•æœ›è¼ƒç‚ºä¿å®ˆã€‚
    ä¸»è¦é—œæ³¨é»åŒ…æ‹¬æ™ºæ…§å‹æ‰‹æ©Ÿå¸‚å ´é£½å’Œã€åœ°ç·£æ”¿æ²»é¢¨éšªæŒçºŒã€‚
    æ­¤å¤–ï¼Œå…¨çƒç¶“æ¿Ÿæˆé•·è¶¨ç·©ä¹Ÿå¯èƒ½å½±éŸ¿åŠå°é«”éœ€æ±‚ã€‚

    æŠ€è¡“é¢åˆ†ææ–¹é¢ï¼Œå°ç©é›»è‚¡åƒ¹åœ¨500å…ƒé™„è¿‘é‡åˆ°å¼·å‹é˜»åŠ›ã€‚
    ä»Šæ—¥æˆäº¤é‡è¼ƒå‰ä¸€äº¤æ˜“æ—¥æ¸›å°‘15%ï¼Œé¡¯ç¤ºæŠ•è³‡äººæŒè§€æœ›æ…‹åº¦ã€‚
    è‹¥èƒ½çªç ´510å…ƒé—œå¡ï¼Œä¸‹ä¸€å€‹æŠ€è¡“ç›®æ¨™åƒ¹ä½å¯èƒ½è½åœ¨530å…ƒã€‚

    å…¬å¸åŒæ™‚å®£å¸ƒåœ¨ç¾åœ‹äºåˆ©æ¡‘é‚£å·ç¬¬äºŒåº§æ™¶åœ“å» çš„å»ºè¨­é€²åº¦ã€‚
    é è¨ˆ2025å¹´é–‹å§‹é‡ç”¢ï¼ŒåˆæœŸæœˆç”¢èƒ½ç‚º2è¬ç‰‡12å‹æ™¶åœ“ã€‚
    é€™é …æŠ•è³‡å°‡æœ‰åŠ©æ–¼å°ç©é›»åˆ†æ•£åœ°ç·£æ”¿æ²»é¢¨éšªï¼Œå¼·åŒ–å…¨çƒä¾›æ‡‰éˆå½ˆæ€§ã€‚
    """

    print(f"æ¸¬è©¦æ–‡ç« é•·åº¦: {len(test_article)} å­—ç¬¦")

    # æ·»åŠ æ¸¬è©¦æ–‡ç« ï¼ˆæœƒè¢«è‡ªå‹•åˆ‡å‰²ï¼‰
    article_metadata = {
        "source": "financial_news",
        "category": "è‚¡å¸‚åˆ†æ",
        "company": "å°ç©é›»",
        "date": "2024-01-15"
    }

    print("\n1. æ·»åŠ æ¸¬è©¦æ–‡ç« ä¸¦é€²è¡Œèªæ„åˆ‡å‰²...")
    result = await store.add_documents_with_semantic_chunking(
        [test_article],
        metadatas=[article_metadata],
        ids=["tsmc_q3_report_2024"]
    )

    print(f"åˆ‡å‰²çµæœ: {result['method']}")
    print(f"ç”¢ç”Ÿ {result['total_chunks']} å€‹èªæ„åˆ‡å‰²ç‰‡æ®µ")
    print(f"å¹³å‡ç‰‡æ®µå¤§å°: {result.get('avg_chunk_size', 0):.0f} å­—ç¬¦")

    # æ¸¬è©¦ä¸åŒçš„æª¢ç´¢æ–¹å¼
    test_queries = [
        ("è²¡å ±", "è²¡å‹™æ•¸æ“šæŸ¥è©¢"),
        ("æ³•èªªæœƒ", "ç®¡ç†å±¤è©•è«–æŸ¥è©¢"),
        ("è‚¡åƒ¹åˆ†æ", "æŠ€è¡“åˆ†ææŸ¥è©¢"),
        ("ç¾åœ‹å·¥å» ", "æŠ•è³‡è¨ˆåŠƒæŸ¥è©¢")
    ]

    article_id = "tsmc_q3_report_2024"

    print("\n2. æ¸¬è©¦åŸºæœ¬æª¢ç´¢ vs ä¸Šä¸‹æ–‡æ“´å±•æª¢ç´¢å°æ¯”")
    print("=" * 60)

    for query, description in test_queries:
        print(f"\næŸ¥è©¢: '{query}' ({description})")
        print("-" * 40)

        # åŸºæœ¬æª¢ç´¢
        print("ğŸ“‹ åŸºæœ¬æª¢ç´¢çµæœ:")
        basic_results = store.search_with_metadata_filtering(
            query, n_results=3, include_metadata=True
        )

        for i, result in enumerate(basic_results, 1):
            article_ref = result.get('metadata', {}).get('original_document_id', 'unknown')
            chunk_idx = result.get('metadata', {}).get('chunk_index', '?')
            print(f"  {i}. æ–‡ç« ID: {article_ref[:20]}..., Chunk: {chunk_idx}")
            print(f"     ç›¸é—œæ€§: {result['relevance_score']:.3f}")
            print(f"     å…§å®¹: {result['document'][:80]}...")

        # ä¸Šä¸‹æ–‡æ“´å±•æª¢ç´¢
        print("\nğŸ”— ä¸Šä¸‹æ–‡æ“´å±•æª¢ç´¢çµæœ:")
        context_results = store.search_with_context_expansion(
            query,
            n_results=3,
            include_article_context=True,
            max_chunks_per_article=3,
            context_similarity_threshold=0.2
        )

        primary_count = sum(1 for r in context_results if r.get('chunk_role') == 'primary')
        context_count = sum(1 for r in context_results if r.get('chunk_role') == 'context')

        print(f"  ç¸½çµæœæ•¸: {len(context_results)} (ä¸»è¦: {primary_count}, ä¸Šä¸‹æ–‡: {context_count})")

        for i, result in enumerate(context_results, 1):
            article_ref = result.get('metadata', {}).get('original_document_id', 'unknown')
            chunk_idx = result.get('metadata', {}).get('chunk_index', '?')
            role = result.get('chunk_role', 'unknown')
            print(f"  {i}. æ–‡ç« ID: {article_ref[:20]}..., Chunk: {chunk_idx}, è§’è‰²: {role}")
            print(f"     ç›¸é—œæ€§: {result['relevance_score']:.3f}")
            print(f"     å…§å®¹: {result['document'][:80]}...")

    print("\n3. æ¸¬è©¦å®Œæ•´æ–‡ç« ä¸Šä¸‹æ–‡æª¢ç´¢")
    print("=" * 40)

    article_context = store.get_article_context(
        article_id,
        include_all_chunks=True,
        sort_by_order=True
    )

    if 'error' not in article_context:
        print(f"æ–‡ç« ID: {article_context['article_id']}")
        print(f"ç¸½ chunks æ•¸: {article_context['total_chunks']}")
        print(f"å¹³å‡èªæ„ä¸€è‡´æ€§: {article_context['avg_semantic_coherence']:.3f}")
        print(f"ä½¿ç”¨çš„åˆ‡å‰²æ–¹å¼: {article_context['chunking_methods_used']}")

        print("\nChunks è©³ç´°è³‡è¨Š:")
        for chunk in article_context['chunks']:
            print(f"  Chunk {chunk['chunk_index']}: {len(chunk['document'])} å­—ç¬¦")
            print(f"    èªæ„ä¸€è‡´æ€§: {chunk['semantic_coherence']:.3f}")
            print(f"    é‚Šç•Œä¿¡å¿ƒåº¦: {chunk['boundary_confidence']:.3f}")
            print(f"    é‡ç–Šé•·åº¦: {chunk['overlap_length']}")
            print(f"    å…§å®¹: {chunk['document'][:60]}...")
            print()

        # æ¸¬è©¦é‡å»ºå®Œæ•´å…§å®¹
        full_content = article_context['full_content']
        print(f"é‡å»ºå…§å®¹é•·åº¦: {len(full_content)} å­—ç¬¦")
        print(f"åŸå§‹å…§å®¹é•·åº¦: {len(test_article)} å­—ç¬¦")
        print(f"å…§å®¹å®Œæ•´æ€§: {len(full_content) / len(test_article) * 100:.1f}%")

    else:
        print(f"ç²å–æ–‡ç« ä¸Šä¸‹æ–‡å¤±æ•—: {article_context['error']}")

    print("\n4. æ¸¬è©¦æ–‡ç« æ„ŸçŸ¥æª¢ç´¢")
    print("=" * 40)

    for query, description in test_queries[:2]:  # æ¸¬è©¦å‰å…©å€‹æŸ¥è©¢
        print(f"\næŸ¥è©¢: '{query}'")

        article_aware_results = store.search_article_aware(
            query,
            n_results=5,
            prioritize_complete_articles=True,
            min_article_coverage=0.5
        )

        if article_aware_results:
            first_result = article_aware_results[0]
            print(f"  æ–‡ç« è©•åˆ†: {first_result.get('article_score', 0):.3f}")
            print(f"  æ–‡ç« è¦†è“‹ç‡: {first_result.get('article_coverage', 0):.1%}")
            print(f"  è¿”å› chunks æ•¸: {len(article_aware_results)}")

            chunks_in_order = sorted(article_aware_results,
                                   key=lambda x: x.get('metadata', {}).get('chunk_index', 0))

            print("  Chunks é †åº:")
            for chunk in chunks_in_order:
                chunk_idx = chunk.get('metadata', {}).get('chunk_index', '?')
                role = chunk.get('chunk_role', 'unknown')
                print(f"    Chunk {chunk_idx} ({role}): {chunk['document'][:50]}...")

    return store


async def test_multiple_articles():
    """æ¸¬è©¦å¤šç¯‡æ–‡ç« çš„é—œè¯æª¢ç´¢"""
    print("\n=== å¤šç¯‡æ–‡ç« é—œè¯æª¢ç´¢æ¸¬è©¦ ===\n")

    store = EnhancedVectorStore(
        collection_name="test_multi_articles",
        enable_semantic_chunking=True
    )

    # å‰µå»ºå¤šç¯‡ç›¸é—œæ–‡ç« 
    articles = [
        {
            "id": "tsmc_earnings",
            "content": "å°ç©é›»å…¬å¸ƒäº®çœ¼è²¡å ±ï¼Œç‡Ÿæ”¶å‰µæ­·å²æ–°é«˜ã€‚ç¬¬ä¸‰å­£ç‡Ÿæ”¶6136å„„å…ƒï¼Œå¹´å¢16.9%ã€‚æ¯›åˆ©ç‡54.3%è¡¨ç¾å„ªç•°ã€‚",
            "metadata": {"company": "å°ç©é›»", "type": "è²¡å ±", "category": "earnings"}
        },
        {
            "id": "tsmc_expansion",
            "content": "å°ç©é›»å®£å¸ƒç¾åœ‹äºåˆ©æ¡‘é‚£å·å·¥å» æ“´å»ºè¨ˆåŠƒã€‚æŠ•è³‡é¡é”400å„„ç¾å…ƒï¼Œé è¨ˆ2025å¹´é‡ç”¢ã€‚æ­¤èˆ‰å°‡åˆ†æ•£åœ°ç·£æ”¿æ²»é¢¨éšªã€‚",
            "metadata": {"company": "å°ç©é›»", "type": "æŠ•è³‡", "category": "expansion"}
        },
        {
            "id": "semiconductor_market",
            "content": "åŠå°é«”å¸‚å ´å±•æœ›æ¨‚è§€ï¼ŒAIæ™¶ç‰‡éœ€æ±‚æŒçºŒå¼·å‹ã€‚å°ç©é›»ã€NVIDIAã€AMDç­‰å…¬å¸å°‡å—æƒ ã€‚é ä¼°æ˜å¹´æˆé•·15%ã€‚",
            "metadata": {"industry": "åŠå°é«”", "type": "å¸‚å ´åˆ†æ", "category": "market"}
        }
    ]

    print(f"æ·»åŠ  {len(articles)} ç¯‡æ¸¬è©¦æ–‡ç« ...")

    # æ·»åŠ æ‰€æœ‰æ–‡ç« 
    for article in articles:
        result = await store.add_documents_with_semantic_chunking(
            [article["content"]],
            metadatas=[article["metadata"]],
            ids=[article["id"]]
        )
        print(f"  {article['id']}: {result['total_chunks']} chunks")

    # æ¸¬è©¦è·¨æ–‡ç« æª¢ç´¢
    print("\næ¸¬è©¦è·¨æ–‡ç« æª¢ç´¢:")

    query = "å°ç©é›»æ¥­ç¸¾åˆ†æ"
    print(f"æŸ¥è©¢: '{query}'")

    # åŸºæœ¬æª¢ç´¢
    basic_results = store.search_with_metadata_filtering(query, n_results=5)
    print(f"\nåŸºæœ¬æª¢ç´¢è¿”å› {len(basic_results)} å€‹çµæœ:")

    article_distribution = {}
    for result in basic_results:
        article_id = result.get('metadata', {}).get('original_document_id', 'unknown')
        article_distribution[article_id] = article_distribution.get(article_id, 0) + 1
        print(f"  æ–‡ç« : {article_id}, ç›¸é—œæ€§: {result['relevance_score']:.3f}")

    print(f"\næ–‡ç« åˆ†ä½ˆ: {article_distribution}")

    # ä¸Šä¸‹æ–‡æ“´å±•æª¢ç´¢
    context_results = store.search_with_context_expansion(
        query,
        n_results=5,
        include_article_context=True,
        max_chunks_per_article=2
    )

    print(f"\nä¸Šä¸‹æ–‡æ“´å±•æª¢ç´¢è¿”å› {len(context_results)} å€‹çµæœ:")

    context_article_distribution = {}
    for result in context_results:
        article_id = result.get('metadata', {}).get('original_document_id', 'unknown')
        role = result.get('chunk_role', 'unknown')
        key = f"{article_id}_{role}"
        context_article_distribution[key] = context_article_distribution.get(key, 0) + 1

    print(f"ä¸Šä¸‹æ–‡æ–‡ç« åˆ†ä½ˆ: {context_article_distribution}")

    return store


async def performance_comparison():
    """æ¯”è¼ƒä¸åŒæª¢ç´¢æ–¹å¼çš„æ€§èƒ½"""
    print("\n=== æª¢ç´¢æ€§èƒ½æ¯”è¼ƒ ===\n")

    store = EnhancedVectorStore(
        collection_name="test_performance",
        enable_semantic_chunking=True
    )

    # å‰µå»ºè¼ƒå¤§çš„æ¸¬è©¦è³‡æ–™é›†
    test_content = """
    å°ç©é›»ä½œç‚ºå…¨çƒæœ€å¤§çš„æ™¶åœ“ä»£å·¥å» ï¼Œåœ¨å…ˆé€²è£½ç¨‹æŠ€è¡“æ–¹é¢ä¿æŒé ˜å…ˆåœ°ä½ã€‚
    å…¬å¸ç¬¬ä¸‰å­£è²¡å ±é¡¯ç¤ºï¼Œ7å¥ˆç±³å’Œ5å¥ˆç±³è£½ç¨‹è²¢ç»äº†è¶…é50%çš„ç‡Ÿæ”¶ã€‚
    éš¨è‘—AIæ™¶ç‰‡éœ€æ±‚æ¿€å¢ï¼Œå°ç©é›»çš„é«˜æ•ˆèƒ½é‹ç®—å¹³å°æ¥­å‹™å¿«é€Ÿæˆé•·ã€‚

    åœ¨åœ°ç·£æ”¿æ²»æ–¹é¢ï¼Œå°ç©é›»ç©æ¥µæ¨å‹•å…¨çƒåŒ–ä½ˆå±€ã€‚
    ç¾åœ‹äºåˆ©æ¡‘é‚£å·çš„å…©åº§12å‹æ™¶åœ“å» å»ºè¨­é€²å±•é †åˆ©ã€‚
    ç¬¬ä¸€åº§å·¥å» é è¨ˆ2024å¹´é–‹å§‹é‡ç”¢5å¥ˆç±³è£½ç¨‹ã€‚
    ç¬¬äºŒåº§å·¥å» å°‡æ–¼2025å¹´é‡ç”¢æ›´å…ˆé€²çš„3å¥ˆç±³è£½ç¨‹ã€‚

    å¸‚å ´åˆ†æå¸«èªç‚ºï¼Œå°ç©é›»åœ¨AIæ™‚ä»£å…·æœ‰ä¸å¯æ›¿ä»£çš„ç«¶çˆ­å„ªå‹¢ã€‚
    å…¬å¸èˆ‡NVIDIAã€AMDã€è˜‹æœç­‰ä¸»è¦å®¢æˆ¶çš„é—œä¿‚ç©©å›ºã€‚
    é ä¼°æœªä¾†ä¸‰å¹´ï¼ŒAIç›¸é—œæ¥­å‹™å°‡ç‚ºå°ç©é›»è²¢ç»30%ä»¥ä¸Šçš„ç‡Ÿæ”¶æˆé•·ã€‚

    ç„¶è€Œï¼ŒæŠ•è³‡è€…ä¹Ÿéœ€è¦é—œæ³¨æ½›åœ¨é¢¨éšªã€‚
    åŒ…æ‹¬ä¸­ç¾ç§‘æŠ€ç«¶çˆ­åŠ åŠ‡ã€å…¨çƒç¶“æ¿Ÿæ”¾ç·©ã€ä»¥åŠæ–°èˆˆç«¶çˆ­è€…çš„å¨è„…ã€‚
    å°ç©é›»å¿…é ˆæŒçºŒæŠ•è³‡ç ”ç™¼ï¼Œç¶­æŒæŠ€è¡“é ˜å…ˆå„ªå‹¢ã€‚
    """ * 3  # é‡è¤‡3æ¬¡ä»¥å¢åŠ å…§å®¹é‡

    print(f"æ¸¬è©¦å…§å®¹é•·åº¦: {len(test_content)} å­—ç¬¦")

    # æ·»åŠ æ¸¬è©¦å…§å®¹
    start_time = time.time()
    result = await store.add_documents_with_semantic_chunking(
        [test_content],
        metadatas=[{"source": "performance_test"}],
        ids=["perf_test_article"]
    )
    indexing_time = time.time() - start_time

    print(f"ç´¢å¼•å»ºç«‹æ™‚é–“: {indexing_time:.3f}ç§’")
    print(f"ç”¢ç”Ÿ chunks: {result['total_chunks']}")

    # æ¸¬è©¦ä¸åŒæª¢ç´¢æ–¹å¼çš„æ€§èƒ½
    test_queries = ["å°ç©é›»è²¡å ±", "AIæ™¶ç‰‡", "ç¾åœ‹å·¥å» ", "ç«¶çˆ­å„ªå‹¢", "æŠ€è¡“é¢¨éšª"]

    print("\næª¢ç´¢æ€§èƒ½æ¸¬è©¦:")
    print("æ–¹å¼\t\tå¹³å‡æ™‚é–“(ms)\tçµæœæ•¸é‡\tä¸Šä¸‹æ–‡å®Œæ•´æ€§")
    print("-" * 60)

    # åŸºæœ¬æª¢ç´¢
    basic_times = []
    basic_results_count = []

    for query in test_queries:
        start = time.time()
        results = store.search_with_metadata_filtering(query, n_results=3)
        elapsed = (time.time() - start) * 1000
        basic_times.append(elapsed)
        basic_results_count.append(len(results))

    avg_basic_time = sum(basic_times) / len(basic_times)
    avg_basic_count = sum(basic_results_count) / len(basic_results_count)

    print(f"åŸºæœ¬æª¢ç´¢\t\t{avg_basic_time:.1f}\t\t{avg_basic_count:.1f}\t\tä½")

    # ä¸Šä¸‹æ–‡æ“´å±•æª¢ç´¢
    context_times = []
    context_results_count = []

    for query in test_queries:
        start = time.time()
        results = store.search_with_context_expansion(query, n_results=3)
        elapsed = (time.time() - start) * 1000
        context_times.append(elapsed)
        context_results_count.append(len(results))

    avg_context_time = sum(context_times) / len(context_times)
    avg_context_count = sum(context_results_count) / len(context_results_count)

    print(f"ä¸Šä¸‹æ–‡æ“´å±•\t\t{avg_context_time:.1f}\t\t{avg_context_count:.1f}\t\té«˜")

    # æ–‡ç« æ„ŸçŸ¥æª¢ç´¢
    article_times = []
    article_results_count = []

    for query in test_queries:
        start = time.time()
        results = store.search_article_aware(query, n_results=3)
        elapsed = (time.time() - start) * 1000
        article_times.append(elapsed)
        article_results_count.append(len(results))

    avg_article_time = sum(article_times) / len(article_times)
    avg_article_count = sum(article_results_count) / len(article_results_count)

    print(f"æ–‡ç« æ„ŸçŸ¥\t\t{avg_article_time:.1f}\t\t{avg_article_count:.1f}\t\tæœ€é«˜")

    print(f"\næ€§èƒ½é–‹éŠ·:")
    print(f"  ä¸Šä¸‹æ–‡æ“´å±•æ¯”åŸºæœ¬æª¢ç´¢æ…¢: {((avg_context_time - avg_basic_time) / avg_basic_time * 100):+.1f}%")
    print(f"  æ–‡ç« æ„ŸçŸ¥æ¯”åŸºæœ¬æª¢ç´¢æ…¢: {((avg_article_time - avg_basic_time) / avg_basic_time * 100):+.1f}%")

    return store


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸ” æ–‡ç« é—œè¯æª¢ç´¢ç³»çµ±æ¸¬è©¦\n")
    print("=" * 60)

    test_functions = [
        ("æ–‡ç« ä¸Šä¸‹æ–‡æª¢ç´¢", test_article_context_retrieval),
        ("å¤šç¯‡æ–‡ç« é—œè¯æª¢ç´¢", test_multiple_articles),
        ("æª¢ç´¢æ€§èƒ½æ¯”è¼ƒ", performance_comparison)
    ]

    for test_name, test_func in test_functions:
        print(f"\nğŸ§ª åŸ·è¡Œæ¸¬è©¦: {test_name}")
        print("-" * 40)

        try:
            start_time = time.time()
            await test_func()
            test_time = time.time() - start_time
            print(f"\nâœ… {test_name} æ¸¬è©¦å®Œæˆ (è€—æ™‚: {test_time:.2f}ç§’)")

        except Exception as e:
            print(f"\nâŒ {test_name} æ¸¬è©¦å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()

        print("\n" + "=" * 60)

    print("\nğŸ‰ æ–‡ç« é—œè¯æª¢ç´¢æ¸¬è©¦å®Œæˆï¼")

    print("\nğŸ“Š åŠŸèƒ½æ‘˜è¦:")
    print("âœ… åŸºæœ¬æª¢ç´¢: å¿«é€Ÿï¼Œä½†å¯èƒ½ä¸Ÿå¤±ä¸Šä¸‹æ–‡")
    print("âœ… ä¸Šä¸‹æ–‡æ“´å±•: è‡ªå‹•åŒ…å«åŒç¯‡æ–‡ç« çš„ç›¸é—œchunks")
    print("âœ… æ–‡ç« æ„ŸçŸ¥æª¢ç´¢: å„ªå…ˆè¿”å›å®Œæ•´æ–‡ç« ï¼Œæœ€ä½³ä¸Šä¸‹æ–‡")
    print("âœ… å®Œæ•´æ–‡ç« æª¢ç´¢: å¯é‡å»ºåŸå§‹æ–‡ç« å…§å®¹")

    print("\nğŸš€ ä½¿ç”¨å»ºè­°:")
    print("- ç°¡å–®äº‹å¯¦æŸ¥è©¢: ä½¿ç”¨åŸºæœ¬æª¢ç´¢")
    print("- éœ€è¦ä¸Šä¸‹æ–‡ç†è§£: ä½¿ç”¨ä¸Šä¸‹æ–‡æ“´å±•æª¢ç´¢")
    print("- è¤‡é›œåˆ†æä»»å‹™: ä½¿ç”¨æ–‡ç« æ„ŸçŸ¥æª¢ç´¢")
    print("- éœ€è¦å®Œæ•´è³‡è¨Š: ä½¿ç”¨å®Œæ•´æ–‡ç« æª¢ç´¢")


if __name__ == "__main__":
    asyncio.run(main())