#!/usr/bin/env python3
"""
RAGç³»çµ±å‡ç´šè…³æœ¬

å¾å‚³çµ±RAGå‡ç´šåˆ°æ–‡ç« æ„ŸçŸ¥RAGï¼š
1. æ¸…ç©ºèˆŠçš„å‘é‡è³‡æ–™åº«
2. ä½¿ç”¨èªæ„åˆ‡å‰²é‡æ–°è¼‰å…¥è²¡ç¶“æ–°è
3. æä¾›å‡ç´šé©—è­‰
"""

import asyncio
import json
import sys
import time
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src" / "main" / "python"))

from dotenv import load_dotenv
load_dotenv()

from src.main.python.rag.enhanced_vector_store import EnhancedVectorStore
from src.main.python.rag.chroma_vector_store import ChromaVectorStore


class RAGUpgrader:
    """RAGç³»çµ±å‡ç´šå™¨"""

    def __init__(self):
        self.old_collection = "finance_knowledge_optimal"
        self.new_collection = "finance_knowledge_enhanced"

    async def backup_and_clear_old_data(self):
        """å‚™ä»½ä¸¦æ¸…ç©ºèˆŠè³‡æ–™"""
        print("=== æ¸…ç†èˆŠè³‡æ–™ ===")

        try:
            # æª¢æŸ¥èˆŠé›†åˆ
            old_store = ChromaVectorStore(collection_name=self.old_collection)
            old_count = old_store.collection.count()
            print(f"èˆŠé›†åˆæ–‡æª”æ•¸ï¼š{old_count}")

            # æ¸…ç©ºæ–°é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            try:
                new_store = EnhancedVectorStore(collection_name=self.new_collection)
                new_store.collection.delete()
                print(f"å·²æ¸…ç©ºæ–°é›†åˆï¼š{self.new_collection}")
            except Exception as e:
                print(f"æ–°é›†åˆä¸å­˜åœ¨æˆ–æ¸…ç©ºå¤±æ•—ï¼š{e}")

            return old_count

        except Exception as e:
            print(f"æ¸…ç†éç¨‹å‡ºç¾éŒ¯èª¤ï¼š{e}")
            return 0

    async def load_cnyes_data_with_semantic_chunking(self):
        """ä½¿ç”¨èªæ„åˆ‡å‰²è¼‰å…¥cnyesæ–°èè³‡æ–™"""
        print("\n=== ä½¿ç”¨èªæ„åˆ‡å‰²è¼‰å…¥æ–°èè³‡æ–™ ===")

        # åˆå§‹åŒ–å¢å¼·å‹å‘é‡å­˜å„²
        enhanced_store = EnhancedVectorStore(
            collection_name=self.new_collection,
            enable_semantic_chunking=True,
            fallback_to_legacy=False
        )

        # è¼‰å…¥åŸå§‹æ–°èè³‡æ–™
        data_file = project_root / "data" / "real_cnyes_news_20250927_132114.json"

        if not data_file.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆï¼š{data_file}")

        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        articles = data.get('articles', [])
        print(f"æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

        # è™•ç†æ–‡ç« 
        documents = []
        metadatas = []

        for i, article in enumerate(articles):
            title = article.get('title', '')
            content = article.get('content', '')
            url = article.get('url', '')
            category = article.get('category', 'è²¡ç¶“')

            # åˆä½µæ¨™é¡Œå’Œå…§å®¹
            full_text = f"æ¨™é¡Œï¼š{title}\nå…§å®¹ï¼š{content}"

            # å‰µå»ºè±å¯Œçš„å…ƒæ•¸æ“š
            metadata = {
                'original_document_id': f"cnyes_article_{i+1}",
                'title': title,
                'url': url,
                'category': category,
                'source': 'cnyes.com',
                'article_index': i + 1,
                'scrape_time': data.get('scrape_time', ''),
                'expert_domain': 'financial_analysis'
            }

            documents.append(full_text)
            metadatas.append(metadata)

        print(f"æº–å‚™è¼‰å…¥ {len(documents)} ç¯‡æ–‡ç« ...")

        # ä½¿ç”¨èªæ„åˆ‡å‰²è¼‰å…¥
        start_time = time.time()
        result = await enhanced_store.add_documents_with_semantic_chunking(
            documents,
            metadatas=metadatas
        )

        load_time = time.time() - start_time

        print(f"è¼‰å…¥å®Œæˆï¼")
        print(f"è™•ç†æ™‚é–“ï¼š{load_time:.2f}ç§’")
        print(f"åˆ‡å‰²æ–¹æ³•ï¼š{result['method']}")
        print(f"ç¸½æ–‡æª”æ•¸ï¼š{result['total_documents']}")
        print(f"ç¸½åˆ‡å‰²ç‰‡æ®µæ•¸ï¼š{result['total_chunks']}")
        print(f"å¹³å‡æ¯æ–‡æª”ç‰‡æ®µæ•¸ï¼š{result['avg_chunks_per_document']:.1f}")

        if 'avg_chunk_size' in result:
            print(f"å¹³å‡ç‰‡æ®µå¤§å°ï¼š{result['avg_chunk_size']:.0f} å­—ç¬¦")
            print(f"å¹³å‡èªæ„ä¸€è‡´æ€§ï¼š{result['avg_semantic_coherence']:.3f}")

        return result

    async def verify_upgrade(self):
        """é©—è­‰å‡ç´šæ•ˆæœ"""
        print("\n=== é©—è­‰å‡ç´šæ•ˆæœ ===")

        enhanced_store = EnhancedVectorStore(collection_name=self.new_collection)

        # æª¢æŸ¥æ•¸æ“šè¼‰å…¥
        document_count = enhanced_store.collection.count()
        print(f"æ–°é›†åˆæ–‡æª”ç¸½æ•¸ï¼š{document_count}")

        # æ¸¬è©¦æ–‡ç« æ„ŸçŸ¥æœç´¢
        test_queries = [
            "å°ç©é›»è²¡å ±åˆ†æ",
            "è¯ç™¼ç§‘å¸‚å ´ç«¶çˆ­",
            "æŠ•è³‡é¢¨éšªè©•ä¼°"
        ]

        print("\næ¸¬è©¦æ–‡ç« æ„ŸçŸ¥æœç´¢ï¼š")
        for query in test_queries:
            print(f"\næŸ¥è©¢ï¼š{query}")

            # å‚³çµ±æœç´¢
            basic_results = enhanced_store.search_similar_documents(query, n_results=1)
            basic_length = len(basic_results[0]['document']) if basic_results else 0

            # æ–‡ç« æ„ŸçŸ¥æœç´¢
            context_results = enhanced_store.search_with_context_expansion(
                query,
                n_results=1,
                include_article_context=True,
                max_context_chunks=3
            )

            context_length = 0
            chunk_count = 0
            if context_results:
                context_length += len(context_results[0]['document'])
                chunk_count += 1

                if 'context_chunks' in context_results[0]:
                    for ctx_chunk in context_results[0]['context_chunks']:
                        context_length += len(ctx_chunk['document'])
                        chunk_count += 1

            print(f"  å‚³çµ±æœç´¢ï¼š{basic_length} å­—ç¬¦")
            print(f"  æ–‡ç« æ„ŸçŸ¥ï¼š{context_length} å­—ç¬¦ ({chunk_count} ç‰‡æ®µ)")
            if basic_length > 0:
                improvement = ((context_length / basic_length) - 1) * 100
                print(f"  æ”¹å–„å¹…åº¦ï¼š{improvement:.1f}%")

    async def run_upgrade(self):
        """åŸ·è¡Œå®Œæ•´å‡ç´šæµç¨‹"""
        print("ğŸš€ RAGç³»çµ±å‡ç´šé–‹å§‹")
        print("=" * 50)

        start_time = time.time()

        try:
            # 1. å‚™ä»½å’Œæ¸…ç†
            old_count = await self.backup_and_clear_old_data()

            # 2. é‡æ–°è¼‰å…¥
            result = await self.load_cnyes_data_with_semantic_chunking()

            # 3. é©—è­‰
            await self.verify_upgrade()

            total_time = time.time() - start_time

            # 4. ç¸½çµ
            print("\n" + "=" * 50)
            print("ğŸ‰ å‡ç´šå®Œæˆç¸½çµ")
            print("=" * 50)
            print(f"èˆŠç³»çµ±æ–‡æª”æ•¸ï¼š{old_count}")
            print(f"æ–°ç³»çµ±åˆ‡å‰²ç‰‡æ®µæ•¸ï¼š{result['total_chunks']}")
            print(f"åˆ‡å‰²æ–¹æ³•ï¼š{result['method']}")
            print(f"ç¸½å‡ç´šæ™‚é–“ï¼š{total_time:.2f}ç§’")

            if 'avg_semantic_coherence' in result:
                print(f"å¹³å‡èªæ„ä¸€è‡´æ€§ï¼š{result['avg_semantic_coherence']:.3f}")

            print("\nâœ… ç³»çµ±å·²æˆåŠŸå‡ç´šåˆ°æ–‡ç« æ„ŸçŸ¥RAG")
            print("ğŸ’¡ é‡æ–°å•Ÿå‹•APIæœå‹™ä»¥ä½¿ç”¨æ–°ç³»çµ±")

        except Exception as e:
            print(f"\nâŒ å‡ç´šå¤±æ•—ï¼š{e}")
            import traceback
            traceback.print_exc()


async def main():
    """ä¸»å‡½æ•¸"""
    upgrader = RAGUpgrader()
    await upgrader.run_upgrade()


if __name__ == "__main__":
    asyncio.run(main())