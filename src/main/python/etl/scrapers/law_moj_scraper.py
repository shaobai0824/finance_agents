"""
å…¨åœ‹æ³•è¦è³‡æ–™åº«çˆ¬èŸ² (law.moj.gov.tw)

å°ˆç‚ºæ³•å¾‹åˆè¦å°ˆå®¶è¨­è¨ˆçš„æ¬Šå¨æ³•è¦è³‡æ–™çˆ¬èŸ²
å„ªå…ˆç´šï¼šç¬¬ä¸€ç´šï¼ˆæ ¸å¿ƒåŸºçŸ³ï¼‰
æŠ€è¡“é›£åº¦ï¼šä½ï¼ˆéœæ…‹é é¢ï¼‰
"""

import re
from typing import List, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from datetime import datetime

from ..base_scraper import BaseScraper, ScrapedArticle, extract_text_from_html


class LawMojScraper(BaseScraper):
    """å…¨åœ‹æ³•è¦è³‡æ–™åº«çˆ¬èŸ²

    çˆ¬å–é‡é»ï¼š
    - æ³•å¾‹æ¢æ–‡
    - æ³•è¦ä¿®æ­£æ­·å²
    - æ–½è¡Œç´°å‰‡
    - æ³•è¦è§£é‡‹
    """

    def __init__(self, config):
        super().__init__(config)
        self.base_url = "https://law.moj.gov.tw"
        self.expert_domain = "legal_compliance"

        # æ³•è¦åˆ†é¡å°æ‡‰
        self.law_categories = {
            "é‡‘èæ³•è¦": ["éŠ€è¡Œæ³•", "è­‰åˆ¸äº¤æ˜“æ³•", "ä¿éšªæ³•", "é‡‘èæ§è‚¡å…¬å¸æ³•"],
            "ç¨…å‹™æ³•è¦": ["æ‰€å¾—ç¨…æ³•", "ç‡Ÿæ¥­ç¨…æ³•", "éºç”¢åŠè´ˆèˆ‡ç¨…æ³•"],
            "æŠ•è³‡æ³•è¦": ["æŠ•è³‡å‹ä¿éšªå•†å“ç®¡ç†è¦å‰‡", "å¢ƒå¤–åŸºé‡‘ç®¡ç†è¾¦æ³•"]
        }

    def scrape(self) -> List[ScrapedArticle]:
        """åŸ·è¡Œæ³•è¦è³‡æ–™çˆ¬èŸ²"""
        self.logger.info(f"é–‹å§‹çˆ¬å– {self.name} - å…¨åœ‹æ³•è¦è³‡æ–™åº«")

        articles = []

        try:
            # 1. çˆ¬å–ä¸»è¦é‡‘èç›¸é—œæ³•è¦
            financial_laws = self._scrape_financial_laws()
            articles.extend(financial_laws)

            # 2. çˆ¬å–æœ€æ–°æ³•è¦ç•°å‹•
            recent_updates = self._scrape_recent_updates()
            articles.extend(recent_updates)

            self.logger.info(f"æˆåŠŸçˆ¬å– {len(articles)} ç­†æ³•è¦è³‡æ–™")

        except Exception as e:
            self.logger.error(f"çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)

        return articles

    def _scrape_financial_laws(self) -> List[ScrapedArticle]:
        """çˆ¬å–é‡‘èç›¸é—œæ³•è¦"""
        articles = []

        for category, law_names in self.law_categories.items():
            self.logger.info(f"çˆ¬å–æ³•è¦åˆ†é¡: {category}")

            for law_name in law_names:
                try:
                    law_articles = self._scrape_law_by_name(law_name, category)
                    articles.extend(law_articles)
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–æ³•è¦ {law_name} å¤±æ•—: {e}")

        return articles

    def _scrape_law_by_name(self, law_name: str, category: str) -> List[ScrapedArticle]:
        """æ ¹æ“šæ³•è¦åç¨±çˆ¬å–å…·é«”æ¢æ–‡"""
        search_url = f"{self.base_url}/LawClass/LawSearchContent.aspx"

        # æ§‹å»ºæœå°‹åƒæ•¸
        search_params = {
            "ty": "ALL",
            "cc": "1,2,3,4,5",
            "kw": law_name
        }

        # å–å¾—æœå°‹çµæœé é¢
        search_page = self.session.get(search_url, params=search_params)
        if search_page.status_code != 200:
            return []

        soup = BeautifulSoup(search_page.content, 'html.parser')

        # æ‰¾åˆ°æ³•è¦é€£çµ
        law_links = self._extract_law_links(soup)

        articles = []
        for link in law_links[:5]:  # é™åˆ¶æ¯å€‹æ³•è¦æœ€å¤šå–5å€‹ç‰ˆæœ¬
            try:
                article = self._scrape_law_content(link, law_name, category)
                if article:
                    articles.append(article)
            except Exception as e:
                self.logger.warning(f"çˆ¬å–æ³•è¦å…§å®¹å¤±æ•— {link}: {e}")

        return articles

    def _extract_law_links(self, soup: BeautifulSoup) -> List[str]:
        """å¾æœå°‹çµæœé é¢æå–æ³•è¦é€£çµ"""
        links = []

        # å°‹æ‰¾æœå°‹çµæœä¸­çš„æ³•è¦é€£çµ
        result_links = soup.find_all('a', href=re.compile(r'LawAll\.aspx\?pcode='))

        for link in result_links:
            href = link.get('href')
            if href:
                full_url = urljoin(self.base_url, href)
                links.append(full_url)

        return links

    def _scrape_law_content(self, url: str, law_name: str, category: str) -> Optional[ScrapedArticle]:
        """çˆ¬å–å…·é«”æ³•è¦å…§å®¹"""
        content = self.get_page_content(url)
        if not content:
            return None

        soup = BeautifulSoup(content, 'html.parser')

        # æå–æ³•è¦æ¨™é¡Œ
        title_element = soup.find('span', {'id': 'ctl00_ContentPlaceHolder1_lblLawName'})
        title = title_element.get_text().strip() if title_element else law_name

        # æå–æ³•è¦å…§å®¹
        content_element = soup.find('div', {'id': 'ctl00_ContentPlaceHolder1_divLawArticle'})
        if not content_element:
            return None

        # æ¸…ç†ä¸¦æå–ç´”æ–‡å­—
        law_text = extract_text_from_html(str(content_element))

        # æå–æ–½è¡Œæ—¥æœŸ
        date_element = soup.find('span', {'id': 'ctl00_ContentPlaceHolder1_lblActDate'})
        publish_date = None
        if date_element:
            date_text = date_element.get_text().strip()
            publish_date = self._parse_taiwan_date(date_text)

        # å»ºç«‹æ–‡ç« ç‰©ä»¶
        article = ScrapedArticle(
            title=title,
            content=self.clean_text(law_text),
            url=url,
            publish_date=publish_date,
            tags=[category, "æ³•è¦", law_name],
            source_website="law.moj.gov.tw",
            expert_domain=self.expert_domain
        )

        return article

    def _scrape_recent_updates(self) -> List[ScrapedArticle]:
        """çˆ¬å–æœ€æ–°æ³•è¦ç•°å‹•"""
        articles = []

        # æ³•è¦ç•°å‹•æŸ¥è©¢é é¢
        updates_url = f"{self.base_url}/LawClass/LawSearchNo.aspx"

        try:
            content = self.get_page_content(updates_url)
            if not content:
                return articles

            soup = BeautifulSoup(content, 'html.parser')

            # æ‰¾åˆ°æœ€æ–°ç•°å‹•æ¸…å–®
            update_links = soup.find_all('a', href=re.compile(r'LawAll\.aspx'))

            for link in update_links[:10]:  # é™åˆ¶æœ€æ–°10ç­†
                try:
                    href = link.get('href')
                    full_url = urljoin(self.base_url, href)

                    article = self._scrape_law_content(full_url, "æ³•è¦ç•°å‹•", "æ³•è¦æ›´æ–°")
                    if article:
                        articles.append(article)

                except Exception as e:
                    self.logger.warning(f"çˆ¬å–æ³•è¦ç•°å‹•å¤±æ•—: {e}")

        except Exception as e:
            self.logger.error(f"çˆ¬å–æœ€æ–°æ³•è¦ç•°å‹•å¤±æ•—: {e}")

        return articles

    def _parse_taiwan_date(self, date_str: str) -> Optional[datetime]:
        """è§£æå°ç£æ°‘åœ‹å¹´ä»½æ—¥æœŸ

        Args:
            date_str: å¦‚ "ä¸­è¯æ°‘åœ‹110å¹´01æœˆ01æ—¥"

        Returns:
            è§£æå¾Œçš„ datetime ç‰©ä»¶
        """
        if not date_str:
            return None

        try:
            # æå–æ°‘åœ‹å¹´ä»½
            year_match = re.search(r'(\d+)å¹´', date_str)
            month_match = re.search(r'(\d+)æœˆ', date_str)
            day_match = re.search(r'(\d+)æ—¥', date_str)

            if year_match and month_match and day_match:
                roc_year = int(year_match.group(1))
                month = int(month_match.group(1))
                day = int(day_match.group(1))

                # æ°‘åœ‹å¹´è½‰è¥¿å…ƒå¹´
                ad_year = roc_year + 1911

                return datetime(ad_year, month, day)

        except (ValueError, AttributeError) as e:
            self.logger.warning(f"è§£æå°ç£æ—¥æœŸå¤±æ•—: {date_str}, éŒ¯èª¤: {e}")

        return None

    def _parse_article_list(self, content: str) -> List[str]:
        """è§£ææ–‡ç« æ¸…å–®é é¢ï¼ˆåŸºé¡è¦æ±‚å¯¦ä½œï¼‰"""
        soup = BeautifulSoup(content, 'html.parser')
        links = []

        for link in soup.find_all('a', href=re.compile(r'LawAll\.aspx')):
            href = link.get('href')
            if href:
                full_url = urljoin(self.base_url, href)
                links.append(full_url)

        return links

    def _parse_article_detail(self, url: str, content: str) -> Optional[ScrapedArticle]:
        """è§£ææ–‡ç« è©³ç´°é é¢ï¼ˆåŸºé¡è¦æ±‚å¯¦ä½œï¼‰"""
        return self._scrape_law_content(url, "æ³•è¦æ¢æ–‡", "æ³•è¦")


def scrape(config: dict) -> List[dict]:
    """çˆ¬èŸ²å…¥å£å‡½æ•¸ï¼Œä¾› ETL æµç¨‹èª¿ç”¨

    Args:
        config: çˆ¬èŸ²è¨­å®š

    Returns:
        çˆ¬å–çµæœçš„å­—å…¸æ¸…å–®
    """
    with LawMojScraper(config) as scraper:
        articles = scraper.scrape()
        return [article.to_dict() for article in articles]


# æ¸¬è©¦å‡½æ•¸
def test_scraper():
    """æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½"""
    test_config = {
        "name": "å…¨åœ‹æ³•è¦è³‡æ–™åº«",
        "expert_domain": "legal_compliance",
        "request_delay": (2, 4),
        "max_retries": 3
    }

    print("ğŸ›ï¸  æ¸¬è©¦å…¨åœ‹æ³•è¦è³‡æ–™åº«çˆ¬èŸ²...")

    with LawMojScraper(test_config) as scraper:
        articles = scraper.scrape()

        print(f"âœ… æˆåŠŸçˆ¬å– {len(articles)} ç­†æ³•è¦è³‡æ–™")

        # é¡¯ç¤ºå‰3ç­†çµæœ
        for i, article in enumerate(articles[:3], 1):
            print(f"\nğŸ“‹ æ³•è¦ {i}:")
            print(f"   æ¨™é¡Œ: {article.title}")
            print(f"   URL: {article.url}")
            print(f"   å…§å®¹é•·åº¦: {len(article.content)} å­—ç¬¦")
            print(f"   æ¨™ç±¤: {', '.join(article.tags)}")


if __name__ == "__main__":
    test_scraper()