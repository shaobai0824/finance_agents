"""
ETL ç®¡ç†å™¨ - è‡ªå‹•åŒ–çˆ¬èŸ²èˆ‡å‘é‡åŒ–æµç¨‹æ§åˆ¶å™¨

å¯¦ä½œ Linus å“²å­¸ï¼š
1. ç°¡æ½”åŸ·å¿µï¼šæ¸…æ™°çš„ ETL æµç¨‹ï¼Œé¿å…éåº¦è¤‡é›œ
2. Never break userspaceï¼šç©©å®šçš„è™•ç†æµç¨‹ï¼Œå®¹éŒ¯æ©Ÿåˆ¶
3. å¯¦ç”¨ä¸»ç¾©ï¼šå°ˆæ³¨è§£æ±ºå¯¦éš›çš„çŸ¥è­˜åº«æ›´æ–°éœ€æ±‚
4. å¥½å“å‘³ï¼šçµ±ä¸€çš„éŒ¯èª¤è™•ç†å’Œç›£æ§
"""

import importlib
import logging
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from pathlib import Path
import asyncio
import sys
import os

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.main.python.rag import ChromaVectorStore
from .base_scraper import ScrapedArticle

logger = logging.getLogger(__name__)


class ETLManager:
    """ETL æµç¨‹ç®¡ç†å™¨

    è² è²¬å”èª¿æ•´å€‹ Extract -> Transform -> Load æµç¨‹
    """

    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ– ETL ç®¡ç†å™¨

        Args:
            config: ETL è¨­å®šå­—å…¸
        """
        self.config = config
        self.vector_store = ChromaVectorStore()

        # ETL è¨­å®š
        self.chunk_size = config.get("chunk_size", 500)
        self.chunk_overlap = config.get("chunk_overlap", 50)
        self.batch_size = config.get("batch_size", 10)

        # çˆ¬èŸ²è¨­å®š
        self.sources = config.get("sources", [])
        self.max_articles_per_source = config.get("max_articles_per_source", 50)

        # æ—¥èªŒè¨­å®š
        self.logger = logging.getLogger("etl.manager")

        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            "total_scraped": 0,
            "total_processed": 0,
            "total_loaded": 0,
            "failed_sources": [],
            "start_time": None,
            "end_time": None
        }

    async def run_full_etl(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„ ETL æµç¨‹

        Returns:
            åŸ·è¡Œçµæœçµ±è¨ˆ
        """
        self.logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œå®Œæ•´ ETL æµç¨‹")
        self.stats["start_time"] = datetime.now()

        try:
            # é€ä¸€è™•ç†æ¯å€‹è³‡æ–™ä¾†æº
            for source_config in self.sources:
                if not source_config.get("enabled", True):
                    self.logger.info(f"â­ï¸  è·³éå·²åœç”¨çš„è³‡æ–™ä¾†æº: {source_config.get('name')}")
                    continue

                await self._process_single_source(source_config)

            self.stats["end_time"] = datetime.now()
            duration = self.stats["end_time"] - self.stats["start_time"]

            self.logger.info(f"ğŸ‰ ETL æµç¨‹å®Œæˆ")
            self.logger.info(f"ğŸ“Š åŸ·è¡Œçµ±è¨ˆ:")
            self.logger.info(f"   çˆ¬å–æ–‡ç« : {self.stats['total_scraped']}")
            self.logger.info(f"   è™•ç†æ–‡æª”: {self.stats['total_processed']}")
            self.logger.info(f"   è¼‰å…¥å‘é‡: {self.stats['total_loaded']}")
            self.logger.info(f"   åŸ·è¡Œæ™‚é–“: {duration}")

            return self.stats

        except Exception as e:
            self.logger.error(f"âŒ ETL æµç¨‹åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
            raise

    async def _process_single_source(self, source_config: Dict[str, Any]):
        """è™•ç†å–®ä¸€è³‡æ–™ä¾†æº

        Args:
            source_config: è³‡æ–™ä¾†æºè¨­å®š
        """
        source_name = source_config.get("name", "Unknown")
        self.logger.info(f"ğŸ”„ è™•ç†è³‡æ–™ä¾†æº: {source_name}")

        try:
            # Step 1: æ“·å– (Extract)
            articles = await self._extract_data(source_config)
            if not articles:
                self.logger.warning(f"âš ï¸  è³‡æ–™ä¾†æº {source_name} æ²’æœ‰çˆ¬å–åˆ°ä»»ä½•æ–‡ç« ")
                return

            self.stats["total_scraped"] += len(articles)
            self.logger.info(f"ğŸ“¥ æˆåŠŸçˆ¬å– {len(articles)} ç¯‡æ–‡ç« ")

            # Step 2: è½‰æ› (Transform)
            processed_chunks = await self._transform_data(articles)
            if not processed_chunks:
                self.logger.warning(f"âš ï¸  è³‡æ–™ä¾†æº {source_name} æ²’æœ‰è™•ç†å‡ºä»»ä½•æ–‡æª”å¡Š")
                return

            self.stats["total_processed"] += len(processed_chunks)
            self.logger.info(f"ğŸ”§ æˆåŠŸè™•ç† {len(processed_chunks)} å€‹æ–‡æª”å¡Š")

            # Step 3: è¼‰å…¥ (Load)
            loaded_count = await self._load_data(processed_chunks, source_config)
            self.stats["total_loaded"] += loaded_count
            self.logger.info(f"ğŸ“¤ æˆåŠŸè¼‰å…¥ {loaded_count} å€‹å‘é‡åˆ°è³‡æ–™åº«")

        except Exception as e:
            self.logger.error(f"âŒ è™•ç†è³‡æ–™ä¾†æº {source_name} å¤±æ•—: {e}", exc_info=True)
            self.stats["failed_sources"].append({
                "name": source_name,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            })

    async def _extract_data(self, source_config: Dict[str, Any]) -> List[ScrapedArticle]:
        """æ“·å–è³‡æ–™ (Extract éšæ®µ)

        Args:
            source_config: è³‡æ–™ä¾†æºè¨­å®š

        Returns:
            çˆ¬å–åˆ°çš„æ–‡ç« æ¸…å–®
        """
        module_name = source_config.get("module")
        if not module_name:
            raise ValueError("è³‡æ–™ä¾†æºè¨­å®šç¼ºå°‘ module æ¬„ä½")

        try:
            # å‹•æ…‹è¼‰å…¥çˆ¬èŸ²æ¨¡çµ„
            scraper_module = importlib.import_module(module_name)

            # å‘¼å«çˆ¬èŸ²çš„ scrape å‡½æ•¸
            if hasattr(scraper_module, 'scrape'):
                raw_results = scraper_module.scrape(source_config)

                # è½‰æ›ç‚º ScrapedArticle ç‰©ä»¶
                articles = []
                for result in raw_results[:self.max_articles_per_source]:
                    if isinstance(result, dict):
                        article = self._dict_to_article(result)
                        if article:
                            articles.append(article)

                return articles

            else:
                raise ImportError(f"çˆ¬èŸ²æ¨¡çµ„ {module_name} æ²’æœ‰ scrape å‡½æ•¸")

        except ImportError as e:
            self.logger.error(f"ç„¡æ³•è¼‰å…¥çˆ¬èŸ²æ¨¡çµ„ {module_name}: {e}")
            raise

    def _dict_to_article(self, data: Dict[str, Any]) -> Optional[ScrapedArticle]:
        """å°‡å­—å…¸è½‰æ›ç‚º ScrapedArticle ç‰©ä»¶"""
        try:
            # è§£æç™¼å¸ƒæ—¥æœŸ
            publish_date = None
            if data.get("publish_date"):
                if isinstance(data["publish_date"], str):
                    try:
                        publish_date = datetime.fromisoformat(data["publish_date"])
                    except ValueError:
                        pass
                elif isinstance(data["publish_date"], datetime):
                    publish_date = data["publish_date"]

            return ScrapedArticle(
                title=data.get("title", ""),
                content=data.get("content", ""),
                url=data.get("url", ""),
                publish_date=publish_date,
                author=data.get("author"),
                tags=data.get("tags", []),
                source_website=data.get("source_website", ""),
                expert_domain=data.get("expert_domain", "")
            )

        except Exception as e:
            self.logger.warning(f"è½‰æ›æ–‡ç« è³‡æ–™å¤±æ•—: {e}")
            return None

    async def _transform_data(self, articles: List[ScrapedArticle]) -> List[Dict[str, Any]]:
        """è½‰æ›è³‡æ–™ (Transform éšæ®µ)

        Args:
            articles: çˆ¬å–åˆ°çš„æ–‡ç« æ¸…å–®

        Returns:
            è™•ç†å¾Œçš„æ–‡æª”å¡Šæ¸…å–®
        """
        processed_chunks = []

        for article in articles:
            try:
                # æ–‡å­—åˆ†å¡Š
                chunks = self._split_text(article.content)

                for i, chunk_text in enumerate(chunks):
                    if len(chunk_text.strip()) < 50:  # éæ¿¾å¤ªçŸ­çš„æ–‡æª”å¡Š
                        continue

                    # ç”Ÿæˆå”¯ä¸€ ID
                    chunk_id = self._generate_chunk_id(article.url, i)

                    chunk_data = {
                        "_id": chunk_id,
                        "content": chunk_text.strip(),
                        "metadata": {
                            "source_url": article.url,
                            "title": article.title,
                            "author": article.author,
                            "publish_date": article.publish_date.isoformat() if article.publish_date else None,
                            "source_website": article.source_website,
                            "expert_domain": article.expert_domain,
                            "tags": article.tags,
                            "chunk_index": i,
                            "total_chunks": len(chunks),
                            "processed_at": datetime.now().isoformat()
                        }
                    }

                    processed_chunks.append(chunk_data)

            except Exception as e:
                self.logger.warning(f"è™•ç†æ–‡ç« å¤±æ•— {article.url}: {e}")

        return processed_chunks

    def _split_text(self, text: str) -> List[str]:
        """åˆ†å‰²æ–‡å­—ç‚ºè¼ƒå°çš„æ–‡æª”å¡Š

        Args:
            text: åŸå§‹æ–‡å­—

        Returns:
            åˆ†å‰²å¾Œçš„æ–‡å­—æ¸…å–®
        """
        if not text:
            return []

        # ç°¡å–®çš„åˆ†å¡Šç­–ç•¥ï¼šæŒ‰æ®µè½å’Œé•·åº¦åˆ†å‰²
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""

        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            # å¦‚æœç•¶å‰å¡ŠåŠ ä¸Šæ–°æ®µè½ä¸æœƒè¶…éé™åˆ¶ï¼Œå°±åˆä½µ
            if len(current_chunk) + len(paragraph) + 2 <= self.chunk_size:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
            else:
                # ç•¶å‰å¡Šå·²æ»¿ï¼Œä¿å­˜ä¸¦é–‹å§‹æ–°å¡Š
                if current_chunk:
                    chunks.append(current_chunk)

                # å¦‚æœå–®å€‹æ®µè½å¤ªé•·ï¼Œéœ€è¦é€²ä¸€æ­¥åˆ†å‰²
                if len(paragraph) > self.chunk_size:
                    sub_chunks = self._split_long_paragraph(paragraph)
                    chunks.extend(sub_chunks[:-1])  # é™¤äº†æœ€å¾Œä¸€å€‹
                    current_chunk = sub_chunks[-1] if sub_chunks else ""
                else:
                    current_chunk = paragraph

        # æ·»åŠ æœ€å¾Œä¸€å€‹å¡Š
        if current_chunk:
            chunks.append(current_chunk)

        return chunks

    def _split_long_paragraph(self, paragraph: str) -> List[str]:
        """åˆ†å‰²éé•·çš„æ®µè½"""
        chunks = []
        words = paragraph.split()
        current_chunk = ""

        for word in words:
            if len(current_chunk) + len(word) + 1 <= self.chunk_size:
                if current_chunk:
                    current_chunk += " " + word
                else:
                    current_chunk = word
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = word

        if current_chunk:
            chunks.append(current_chunk)

        return chunks

    def _generate_chunk_id(self, url: str, chunk_index: int) -> str:
        """ç”Ÿæˆæ–‡æª”å¡Šçš„å”¯ä¸€ ID

        Args:
            url: åŸå§‹æ–‡ç«  URL
            chunk_index: æ–‡æª”å¡Šç´¢å¼•

        Returns:
            å”¯ä¸€ ID
        """
        content = f"{url}_{chunk_index}"
        hash_obj = hashlib.md5(content.encode())
        return f"chunk_{hash_obj.hexdigest()[:12]}"

    async def _load_data(self, chunks: List[Dict[str, Any]], source_config: Dict[str, Any]) -> int:
        """è¼‰å…¥è³‡æ–™ (Load éšæ®µ)

        Args:
            chunks: è™•ç†å¾Œçš„æ–‡æª”å¡Šæ¸…å–®
            source_config: è³‡æ–™ä¾†æºè¨­å®š

        Returns:
            æˆåŠŸè¼‰å…¥çš„æ–‡æª”æ•¸é‡
        """
        if not chunks:
            return 0

        try:
            # æº–å‚™æ‰¹æ¬¡è¼‰å…¥çš„æ–‡ä»¶å’Œå…ƒè³‡æ–™
            documents = []
            metadatas = []
            ids = []

            for chunk in chunks:
                documents.append(chunk["content"])
                metadatas.append(chunk["metadata"])
                ids.append(chunk["_id"])

            # æ‰¹æ¬¡è¼‰å…¥åˆ°å‘é‡è³‡æ–™åº«
            loaded_ids = self.vector_store.add_documents(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )

            return len(loaded_ids)

        except Exception as e:
            self.logger.error(f"è¼‰å…¥è³‡æ–™åˆ°å‘é‡è³‡æ–™åº«å¤±æ•—: {e}")
            raise

    def get_stats(self) -> Dict[str, Any]:
        """å–å¾— ETL åŸ·è¡Œçµ±è¨ˆ"""
        return self.stats.copy()

    async def test_single_source(self, source_name: str) -> Dict[str, Any]:
        """æ¸¬è©¦å–®ä¸€è³‡æ–™ä¾†æº

        Args:
            source_name: è³‡æ–™ä¾†æºåç¨±

        Returns:
            æ¸¬è©¦çµæœ
        """
        source_config = None
        for config in self.sources:
            if config.get("name") == source_name:
                source_config = config
                break

        if not source_config:
            raise ValueError(f"æ‰¾ä¸åˆ°è³‡æ–™ä¾†æº: {source_name}")

        self.logger.info(f"ğŸ§ª æ¸¬è©¦è³‡æ–™ä¾†æº: {source_name}")

        # é‡ç½®çµ±è¨ˆ
        self.stats = {
            "total_scraped": 0,
            "total_processed": 0,
            "total_loaded": 0,
            "failed_sources": [],
            "start_time": datetime.now(),
            "end_time": None
        }

        await self._process_single_source(source_config)

        self.stats["end_time"] = datetime.now()
        return self.stats


# ç¨ç«‹çš„ ETL åŸ·è¡Œè…³æœ¬å‡½æ•¸
async def run_etl_from_config(config_path: str):
    """å¾è¨­å®šæª”åŸ·è¡Œ ETL æµç¨‹

    Args:
        config_path: è¨­å®šæª”è·¯å¾‘
    """
    import yaml

    # è¼‰å…¥è¨­å®š
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # å»ºç«‹ ETL ç®¡ç†å™¨
    etl_manager = ETLManager(config)

    # åŸ·è¡Œ ETL æµç¨‹
    return await etl_manager.run_full_etl()


if __name__ == "__main__":
    # æ¸¬è©¦ç”¨çš„ç°¡å–®è¨­å®š
    test_config = {
        "sources": [
            {
                "name": "å…¨åœ‹æ³•è¦è³‡æ–™åº«æ¸¬è©¦",
                "module": "src.main.python.etl.scrapers.law_moj_scraper",
                "expert_domain": "legal_compliance",
                "enabled": True
            }
        ],
        "chunk_size": 500,
        "chunk_overlap": 50,
        "max_articles_per_source": 5
    }

    async def test_etl():
        etl_manager = ETLManager(test_config)
        stats = await etl_manager.test_single_source("å…¨åœ‹æ³•è¦è³‡æ–™åº«æ¸¬è©¦")
        print(f"ğŸ“Š æ¸¬è©¦çµæœ: {stats}")

    # åŸ·è¡Œæ¸¬è©¦
    asyncio.run(test_etl())