#!/usr/bin/env python3
"""
é©—è­‰ LLM ä¸­ä»‹å±¤æ­£å¸¸å·¥ä½œ
ç¢ºä¿æ‰€æœ‰ agents éƒ½é€šé LLM è€Œä¸æ˜¯é è¨­å›æ‡‰
"""

import asyncio
import sys
import os

# Setup path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src', 'main', 'python'))

async def test_llm_middleware():
    """æ¸¬è©¦ LLM ä¸­ä»‹å±¤"""
    print("=" * 60)
    print("LLM ä¸­ä»‹å±¤é©—è­‰æ¸¬è©¦")
    print("=" * 60)

    # 1. æ¸¬è©¦ LLM å®¢æˆ¶ç«¯
    print("\n1. æª¢æŸ¥ LLM å®¢æˆ¶ç«¯é…ç½®")
    try:
        from llm import llm_manager, is_llm_configured, is_real_llm_configured

        print(f"   LLM å®¢æˆ¶ç«¯å·²é…ç½®: {is_llm_configured()}")
        print(f"   çœŸå¯¦ LLM å·²é…ç½®: {is_real_llm_configured()}")
        print(f"   å¯ç”¨å®¢æˆ¶ç«¯: {llm_manager.get_available_clients()}")
        print(f"   é è¨­å®¢æˆ¶ç«¯: {llm_manager.default_client}")

    except Exception as e:
        print(f"   âŒ LLM å®¢æˆ¶ç«¯æª¢æŸ¥å¤±æ•—: {e}")
        return False

    # 2. æ¸¬è©¦ç›´æ¥ LLM èª¿ç”¨
    print("\n2. æ¸¬è©¦ç›´æ¥ LLM èª¿ç”¨")
    try:
        from llm import generate_llm_response

        response = await generate_llm_response("è«‹æä¾›æŠ•è³‡å»ºè­°", max_tokens=100)
        print(f"   âœ… LLM å›æ‡‰æˆåŠŸ")
        print(f"   æ¨¡å‹: {response.model}")
        print(f"   å…§å®¹é•·åº¦: {len(response.content)} å­—å…ƒ")
        print(f"   å®ŒæˆåŸå› : {response.finish_reason}")
        print(f"   å›æ‡‰æ™‚é–“: {response.response_time:.2f} ç§’")

    except Exception as e:
        print(f"   âŒ ç›´æ¥ LLM èª¿ç”¨å¤±æ•—: {e}")
        return False

    # 3. æ¸¬è©¦ agents åˆå§‹åŒ–å’Œ LLM ç‹€æ…‹
    print("\n3. æ¸¬è©¦ agents LLM ç‹€æ…‹")
    try:
        from agents import (
            ManagerAgent,
            FinancialAnalystAgentLLM,
            FinancialPlannerAgentLLM,
            LegalExpertAgentLLM
        )

        agents = {
            'manager': ManagerAgent(),
            'financial_analyst': FinancialAnalystAgentLLM(),
            'financial_planner': FinancialPlannerAgentLLM(),
            'legal_expert': LegalExpertAgentLLM()
        }

        for name, agent in agents.items():
            llm_status = agent.get_llm_status()
            print(f"   {name}:")
            print(f"     LLM é…ç½®: {llm_status['llm_configured']}")
            print(f"     æ¨¡å‹: {llm_status['model']}")
            print(f"     ä½¿ç”¨ RAG: {llm_status['use_rag']}")

    except Exception as e:
        print(f"   âŒ Agents LLM ç‹€æ…‹æª¢æŸ¥å¤±æ•—: {e}")
        return False

    # 4. æ¸¬è©¦ agent è¨Šæ¯è™•ç†æ˜¯å¦ä½¿ç”¨ LLM
    print("\n4. æ¸¬è©¦ agent è¨Šæ¯è™•ç†")
    try:
        from agents.base_agent import AgentMessage, MessageType, AgentType

        # æ¸¬è©¦ç†è²¡å°ˆå®¶
        planner = agents['financial_planner']
        message = AgentMessage(
            agent_type=AgentType.FINANCIAL_PLANNER,
            message_type=MessageType.QUERY,
            content='è«‹çµ¦æˆ‘æŠ•è³‡å»ºè­°'
        )

        print(f"   ç™¼é€è¨Šæ¯çµ¦: {planner.name}")
        response = await planner.process_message(message)

        print(f"   âœ… è¨Šæ¯è™•ç†æˆåŠŸ")
        print(f"   å›æ‡‰é¡å‹: {response.message_type.value}")
        print(f"   ä¿¡å¿ƒåº¦: {response.confidence}")
        print(f"   æ˜¯å¦ä½¿ç”¨ LLM: {response.metadata.get('llm_model', 'unknown')}")
        print(f"   å…§å®¹é•·åº¦: {len(response.content)} å­—å…ƒ")

        # æª¢æŸ¥å›æ‡‰æ˜¯å¦ä¾†è‡ª LLM è€Œéé è¨­æ¨¡æ¿
        if "æ¨¡æ“¬ LLM å›æ‡‰" in response.content or "é è¨­" in response.content:
            print(f"   âš ï¸  å¯èƒ½ä½¿ç”¨äº† fallback å›æ‡‰")
        else:
            print(f"   âœ… ä½¿ç”¨äº† LLM ä¸­ä»‹å±¤ç”Ÿæˆå›æ‡‰")

    except Exception as e:
        print(f"   âŒ Agent è¨Šæ¯è™•ç†æ¸¬è©¦å¤±æ•—: {e}")
        return False

    # 5. é©—è­‰æ²’æœ‰ç›´æ¥ä½¿ç”¨é è¨­å›æ‡‰
    print("\n5. é©—è­‰ LLM ä¸­ä»‹å±¤ä½¿ç”¨")

    # æ¸¬è©¦å¤šå€‹æŸ¥è©¢ç¢ºä¿éƒ½é€šé LLM
    test_queries = [
        "è‚¡ç¥¨æŠ•è³‡å»ºè­°",
        "å¸‚å ´åˆ†æå ±å‘Š",
        "æ³•å¾‹åˆè¦å•é¡Œ"
    ]

    all_using_llm = True

    for query in test_queries:
        try:
            analyst = agents['financial_analyst']
            message = AgentMessage(
                agent_type=AgentType.FINANCIAL_ANALYST,
                message_type=MessageType.QUERY,
                content=query
            )

            response = await analyst.process_message(message)

            # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ LLM
            llm_model = response.metadata.get('llm_model', 'unknown')
            if llm_model == 'unknown' or "é è¨­" in response.content:
                print(f"   âŒ æŸ¥è©¢ '{query}' å¯èƒ½æ²’æœ‰ä½¿ç”¨ LLM")
                all_using_llm = False
            else:
                print(f"   âœ… æŸ¥è©¢ '{query}' ä½¿ç”¨äº† LLM ({llm_model})")

        except Exception as e:
            print(f"   âŒ æŸ¥è©¢ '{query}' è™•ç†å¤±æ•—: {e}")
            all_using_llm = False

    return all_using_llm

async def main():
    """ä¸»å‡½æ•¸"""
    print("é–‹å§‹é©—è­‰ LLM ä¸­ä»‹å±¤...")

    success = await test_llm_middleware()

    print("\n" + "=" * 60)
    print("é©—è­‰çµæœ")
    print("=" * 60)

    if success:
        print("âœ… æˆåŠŸ: æ‰€æœ‰ agents éƒ½æ­£ç¢ºä½¿ç”¨ LLM ä¸­ä»‹å±¤")
        print("âœ… æ¨¡å‹ç‰ˆæœ¬: gpt-4o-mini")
        print("âœ… æ²’æœ‰ä½¿ç”¨é è¨­å›æ‡‰")
        print("\nğŸ‰ LLM ä¸­ä»‹å±¤é…ç½®æ­£ç¢º!")
    else:
        print("âŒ å¤±æ•—: ç™¼ç¾å•é¡Œéœ€è¦ä¿®å¾©")
        print("âŒ éƒ¨åˆ† agents å¯èƒ½ä»åœ¨ä½¿ç”¨é è¨­å›æ‡‰")

    return success

if __name__ == "__main__":
    result = asyncio.run(main())
    sys.exit(0 if result else 1)