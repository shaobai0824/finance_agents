#!/usr/bin/env python3
"""
RAGç³»çµ±å°æ¯”æ¸¬è©¦ï¼šTraditional vs Article-Aware
å°ˆé–€æ¸¬è©¦è²¡ç¶“æ–°èç†è§£çš„å·®ç•°
"""

import asyncio
import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Tuple

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src" / "main" / "python"))

from dotenv import load_dotenv
load_dotenv()

from src.main.python.rag.enhanced_vector_store import EnhancedVectorStore
from src.main.python.rag.chroma_vector_store import ChromaVectorStore


class RAGComparisonTester:
    """RAGç³»çµ±å°æ¯”æ¸¬è©¦å™¨"""

    def __init__(self):
        self.test_scenarios = self._create_financial_scenarios()

    def _create_financial_scenarios(self) -> List[Dict[str, Any]]:
        """å‰µå»ºè²¡ç¶“æ–°èæ¸¬è©¦å ´æ™¯"""
        return [
            {
                "scenario": "å°ç©é›»è²¡å ±å½±éŸ¿åˆ†æ",
                "article": """
                å°ç©é›»å…¬å¸ƒç¬¬ä¸‰å­£è²¡å ±ï¼Œç‡Ÿæ”¶é”æ–°å°å¹£6,136å„„å…ƒï¼Œè¼ƒå‰å­£æˆé•·14.6%ï¼Œè¼ƒå»å¹´åŒæœŸæˆé•·16.9%ã€‚
                æ¯›åˆ©ç‡ç‚º54.3%ï¼Œç‡Ÿæ¥­åˆ©ç›Šç‡ç‚º45.1%ï¼Œç¨…å¾Œæ·¨åˆ©ç‚º1,548å„„å…ƒï¼Œæ¯è‚¡ç›ˆé¤˜ç‚º5.97å…ƒã€‚

                ç„¶è€Œï¼Œæ³•äººåˆ†æå¸«å°æ–¼æ˜å¹´ç¬¬ä¸€å­£çš„å±•æœ›è¼ƒç‚ºä¿å®ˆã€‚
                ä¸»è¦é—œæ³¨é»åŒ…æ‹¬æ™ºæ…§å‹æ‰‹æ©Ÿéœ€æ±‚æ”¾ç·©ã€åœ°ç·£æ”¿æ²»é¢¨éšªæŒçºŒï¼Œä»¥åŠå…¨çƒç¶“æ¿Ÿæˆé•·è¶¨ç·©çš„å½±éŸ¿ã€‚
                æ‘©æ ¹å£«ä¸¹åˆ©åˆ†æå¸«èªç‚ºï¼Œå°ç©é›»é¢è‡¨çš„ä¸»è¦æŒ‘æˆ°æ˜¯è˜‹æœè¨‚å–®æ¸›å°‘å’Œä¸­åœ‹å¸‚å ´çš„ä¸ç¢ºå®šæ€§ã€‚

                æŠ€è¡“é¢åˆ†æé¡¯ç¤ºï¼Œå°ç©é›»è‚¡åƒ¹åœ¨500å…ƒé™„è¿‘é‡åˆ°é˜»åŠ›ã€‚
                æˆäº¤é‡è¼ƒå‰ä¸€å€‹äº¤æ˜“æ—¥æ¸›å°‘15%ï¼Œé¡¯ç¤ºæŠ•è³‡äººæŒè§€æœ›æ…‹åº¦ã€‚
                è‹¥èƒ½çªç ´510å…ƒé—œå¡ï¼Œä¸‹ä¸€å€‹ç›®æ¨™åƒ¹ä½å¯èƒ½è½åœ¨530å…ƒã€‚
                ç„¶è€Œï¼Œè‹¥è·Œç ´480å…ƒæ”¯æ’ï¼Œå¯èƒ½æ¸¬è©¦460å…ƒçš„é‡è¦æ”¯æ’ä½ã€‚

                æ­¤å¤–ï¼Œå°ç©é›»å®£å¸ƒå°‡åœ¨ç¾åœ‹äºåˆ©æ¡‘é‚£å·èˆˆå»ºç¬¬äºŒåº§æ™¶åœ“å» ã€‚
                é è¨ˆ2025å¹´é–‹å§‹é‡ç”¢ï¼ŒåˆæœŸæœˆç”¢èƒ½ç‚º2è¬ç‰‡12å‹æ™¶åœ“ã€‚
                é€™é …æŠ•è³‡å°‡æœ‰åŠ©æ–¼å°ç©é›»åˆ†æ•£åœ°ç·£æ”¿æ²»é¢¨éšªï¼Œå¼·åŒ–ä¾›æ‡‰éˆå½ˆæ€§ã€‚
                ä½†åˆ†æå¸«æ“”å¿ƒï¼Œç¾åœ‹å» çš„æˆæœ¬çµæ§‹å¯èƒ½å½±éŸ¿æ•´é«”ç²åˆ©èƒ½åŠ›ã€‚
                """,
                "queries": [
                    "å°ç©é›»Q3è²¡å ±è¡¨ç¾å¦‚ä½•ï¼Ÿ",
                    "åˆ†æå¸«å°å°ç©é›»çš„çœ‹æ³•æ˜¯ä»€éº¼ï¼Ÿ",
                    "å°ç©é›»è‚¡åƒ¹æŠ€è¡“é¢å¦‚ä½•åˆ†æï¼Ÿ",
                    "å°ç©é›»æŠ•è³‡é¢¨éšªæœ‰å“ªäº›ï¼Ÿ",
                    "å°ç©é›»ç¾åœ‹è¨­å» å°å…¬å¸çš„å½±éŸ¿ï¼Ÿ"
                ]
            },
            {
                "scenario": "è¯ç™¼ç§‘ç«¶çˆ­åˆ†æ",
                "article": """
                è¯ç™¼ç§‘æŠ€ç™¼å¸ƒæ–°ä¸€ä»£å¤©ç’£9400è™•ç†å™¨ï¼Œæ¡ç”¨å°ç©é›»3å¥ˆç±³è£½ç¨‹ï¼Œæ•ˆèƒ½è¼ƒå‰ä»£æå‡30%ã€‚
                æ–°æ™¶ç‰‡åœ¨AIé‹ç®—å’Œå½±åƒè™•ç†æ–¹é¢æœ‰é¡¯è‘—æ”¹é€²ï¼Œé æœŸå°‡æ¶æ”»é«˜éšæ‰‹æ©Ÿå¸‚å ´ã€‚

                å¸‚å ´ç ”ç©¶æ©Ÿæ§‹CounterpointæŒ‡å‡ºï¼Œè¯ç™¼ç§‘åœ¨å…¨çƒæ™ºæ…§å‹æ‰‹æ©Ÿæ™¶ç‰‡å¸‚å ´ä½”æœ‰ç‡é”32%ã€‚
                ä¸»è¦ç«¶çˆ­å°æ‰‹é«˜é€šçš„å¸‚ä½”ç‡ç‚º35%ï¼Œå…©è€…å·®è·æ­£åœ¨ç¸®å°ã€‚
                è¯ç™¼ç§‘çš„å„ªå‹¢åœ¨æ–¼åƒ¹æ ¼ç«¶çˆ­åŠ›å’Œä¸­éšå¸‚å ´çš„å¼·å‹¢åœ°ä½ã€‚

                è²¡å‹™è¡¨ç¾æ–¹é¢ï¼Œè¯ç™¼ç§‘ç¬¬ä¸‰å­£ç‡Ÿæ”¶1,425å„„å…ƒï¼Œå­£å¢8.9%ï¼Œæ¯›åˆ©ç‡ç¶­æŒåœ¨48.5%ã€‚
                è‘£äº‹é•·è”¡æ˜ä»‹è¡¨ç¤ºï¼Œå…¬å¸æŒçºŒæŠ•è³‡5Gå’ŒAIæŠ€è¡“ï¼Œç ”ç™¼è²»ç”¨ä½”ç‡Ÿæ”¶æ¯”é‡é”22%ã€‚

                ç„¶è€Œï¼Œä¾›æ‡‰éˆç®¡ç†é¢è‡¨æŒ‘æˆ°ã€‚ä¸­ç¾ç§‘æŠ€æˆ°å½±éŸ¿ä¸‹ï¼Œè¯ç™¼ç§‘éœ€è¦èª¿æ•´ä¾›æ‡‰å•†ç­–ç•¥ã€‚
                åˆ†æå¸«èªç‚ºï¼Œåœ°ç·£æ”¿æ²»é¢¨éšªæ˜¯è¯ç™¼ç§‘æœªä¾†ç™¼å±•çš„ä¸»è¦ä¸ç¢ºå®šå› ç´ ã€‚
                æ­¤å¤–ï¼Œæ™ºæ…§å‹æ‰‹æ©Ÿå¸‚å ´æˆé•·è¶¨ç·©ï¼Œè¯ç™¼ç§‘æ­£ç©æ¥µå¸ƒå±€è»Šç”¨æ™¶ç‰‡å’Œç‰©è¯ç¶²é ˜åŸŸã€‚
                """,
                "queries": [
                    "è¯ç™¼ç§‘æ–°è™•ç†å™¨æœ‰ä»€éº¼ç‰¹è‰²ï¼Ÿ",
                    "è¯ç™¼ç§‘èˆ‡é«˜é€šçš„ç«¶çˆ­æ…‹å‹¢å¦‚ä½•ï¼Ÿ",
                    "è¯ç™¼ç§‘çš„è²¡å‹™è¡¨ç¾æ€éº¼æ¨£ï¼Ÿ",
                    "è¯ç™¼ç§‘é¢è‡¨å“ªäº›æŒ‘æˆ°ï¼Ÿ",
                    "è¯ç™¼ç§‘å¦‚ä½•æ‡‰å°å¸‚å ´è®ŠåŒ–ï¼Ÿ"
                ]
            }
        ]

    async def test_traditional_rag(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸¬è©¦å‚³çµ±RAGç³»çµ±"""
        print(f"\n=== æ¸¬è©¦å‚³çµ±RAGï¼š{scenario['scenario']} ===")

        # åˆå§‹åŒ–å‚³çµ±å‘é‡å­˜å„²
        traditional_store = ChromaVectorStore(
            collection_name="traditional_test",
            persist_directory="data/chroma_traditional_test"
        )

        # æ¨¡æ“¬å‚³çµ±å›ºå®šå¤§å°åˆ‡å‰²
        article_text = scenario['article']
        chunks = self._traditional_chunking(article_text, chunk_size=400)

        print(f"å‚³çµ±åˆ‡å‰²ç”¢ç”Ÿ {len(chunks)} å€‹ç‰‡æ®µ")

        # æ·»åŠ æ–‡æª”
        await traditional_store.add_documents(
            chunks,
            metadatas=[{"source": scenario['scenario'], "chunk_id": i} for i in range(len(chunks))]
        )

        results = {}
        for query in scenario['queries']:
            print(f"\næŸ¥è©¢ï¼š{query}")

            # åªè¿”å›æœ€ç›¸é—œçš„å–®ä¸€ç‰‡æ®µ
            search_results = traditional_store.search_similar_documents(query, n_results=1)

            result_text = search_results[0]['document'] if search_results else "ç„¡çµæœ"
            results[query] = {
                'content': result_text,
                'chunk_count': 1,
                'total_length': len(result_text)
            }

            print(f"è¿”å›å…§å®¹é•·åº¦ï¼š{len(result_text)} å­—ç¬¦")
            print(f"å…§å®¹é è¦½ï¼š{result_text[:100]}...")

        return results

    async def test_article_aware_rag(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸¬è©¦æ–‡ç« æ„ŸçŸ¥RAGç³»çµ±"""
        print(f"\n=== æ¸¬è©¦æ–‡ç« æ„ŸçŸ¥RAGï¼š{scenario['scenario']} ===")

        # åˆå§‹åŒ–å¢å¼·å‹å‘é‡å­˜å„²
        enhanced_store = EnhancedVectorStore(
            collection_name="enhanced_test",
            enable_semantic_chunking=True,
            fallback_to_legacy=False
        )

        print("ä½¿ç”¨èªæ„åˆ‡å‰²æ·»åŠ æ–‡æª”...")

        # ä½¿ç”¨èªæ„åˆ‡å‰²å’Œæ–‡ç« æ„ŸçŸ¥
        add_result = await enhanced_store.add_documents_with_semantic_chunking(
            [scenario['article']],
            metadatas=[{"source": scenario['scenario'], "original_document_id": f"doc_{scenario['scenario']}"}]
        )

        print(f"èªæ„åˆ‡å‰²ç”¢ç”Ÿ {add_result['total_chunks']} å€‹ç‰‡æ®µ")

        results = {}
        for query in scenario['queries']:
            print(f"\næŸ¥è©¢ï¼š{query}")

            # ä½¿ç”¨ä¸Šä¸‹æ–‡æ“´å±•æœå°‹
            search_results = enhanced_store.search_with_context_expansion(
                query,
                n_results=2,
                include_article_context=True,
                max_context_chunks=3
            )

            # åˆä½µæ‰€æœ‰ç›¸é—œå…§å®¹
            all_content = []
            chunk_count = 0

            for result in search_results:
                all_content.append(result['document'])
                chunk_count += 1

                # æ·»åŠ ä¸Šä¸‹æ–‡chunks
                if 'context_chunks' in result:
                    for context_chunk in result['context_chunks']:
                        all_content.append(context_chunk['document'])
                        chunk_count += 1

            combined_content = "\n\n".join(all_content)

            results[query] = {
                'content': combined_content,
                'chunk_count': chunk_count,
                'total_length': len(combined_content)
            }

            print(f"è¿”å›ç‰‡æ®µæ•¸ï¼š{chunk_count}")
            print(f"ç¸½å…§å®¹é•·åº¦ï¼š{len(combined_content)} å­—ç¬¦")
            print(f"å…§å®¹é è¦½ï¼š{combined_content[:100]}...")

        return results

    def _traditional_chunking(self, text: str, chunk_size: int = 400) -> List[str]:
        """æ¨¡æ“¬å‚³çµ±å›ºå®šå¤§å°åˆ‡å‰²"""
        chunks = []
        sentences = text.split('ã€‚')

        current_chunk = ""
        for sentence in sentences:
            if len(current_chunk + sentence + 'ã€‚') <= chunk_size:
                current_chunk += sentence + 'ã€‚'
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + 'ã€‚'

        if current_chunk:
            chunks.append(current_chunk.strip())

        return [chunk for chunk in chunks if len(chunk.strip()) > 0]

    def analyze_comparison(self, traditional_results: Dict[str, Any],
                          enhanced_results: Dict[str, Any],
                          scenario: str) -> Dict[str, Any]:
        """åˆ†æå°æ¯”çµæœ"""

        analysis = {
            'scenario': scenario,
            'query_comparisons': {},
            'summary': {}
        }

        total_traditional_length = 0
        total_enhanced_length = 0
        total_traditional_chunks = 0
        total_enhanced_chunks = 0

        for query in traditional_results.keys():
            trad = traditional_results[query]
            enh = enhanced_results[query]

            # åˆ†æå…§å®¹è±å¯Œåº¦
            content_richness_ratio = enh['total_length'] / trad['total_length'] if trad['total_length'] > 0 else 0

            # åˆ†æé—œéµè©è¦†è“‹
            query_keywords = set(query.replace('ï¼Ÿ', '').replace('?', '').split())
            trad_coverage = sum(1 for keyword in query_keywords if keyword in trad['content'])
            enh_coverage = sum(1 for keyword in query_keywords if keyword in enh['content'])

            analysis['query_comparisons'][query] = {
                'traditional': {
                    'length': trad['total_length'],
                    'chunks': trad['chunk_count'],
                    'keyword_coverage': trad_coverage
                },
                'enhanced': {
                    'length': enh['total_length'],
                    'chunks': enh['chunk_count'],
                    'keyword_coverage': enh_coverage
                },
                'improvement': {
                    'content_richness': content_richness_ratio,
                    'additional_chunks': enh['chunk_count'] - trad['chunk_count'],
                    'keyword_improvement': enh_coverage - trad_coverage
                }
            }

            total_traditional_length += trad['total_length']
            total_enhanced_length += enh['total_length']
            total_traditional_chunks += trad['chunk_count']
            total_enhanced_chunks += enh['chunk_count']

        # æ•´é«”æ‘˜è¦
        analysis['summary'] = {
            'avg_content_increase': (total_enhanced_length / total_traditional_length - 1) * 100 if total_traditional_length > 0 else 0,
            'avg_chunk_increase': (total_enhanced_chunks / total_traditional_chunks - 1) * 100 if total_traditional_chunks > 0 else 0,
            'total_queries': len(traditional_results),
            'traditional_avg_length': total_traditional_length / len(traditional_results),
            'enhanced_avg_length': total_enhanced_length / len(traditional_results)
        }

        return analysis

    async def run_comprehensive_test(self):
        """åŸ·è¡Œå®Œæ•´å°æ¯”æ¸¬è©¦"""
        print("ğŸ”¬ è²¡ç¶“æ–°èRAGç³»çµ±å°æ¯”æ¸¬è©¦")
        print("=" * 60)

        all_analyses = []

        for scenario in self.test_scenarios:
            print(f"\nğŸ“° æ¸¬è©¦å ´æ™¯ï¼š{scenario['scenario']}")
            print("-" * 40)

            # æ¸¬è©¦å‚³çµ±RAG
            traditional_results = await self.test_traditional_rag(scenario)

            # æ¸¬è©¦æ–‡ç« æ„ŸçŸ¥RAG
            enhanced_results = await self.test_article_aware_rag(scenario)

            # åˆ†æå°æ¯”
            analysis = self.analyze_comparison(
                traditional_results,
                enhanced_results,
                scenario['scenario']
            )

            all_analyses.append(analysis)

            # è¼¸å‡ºå ´æ™¯åˆ†æ
            self._print_scenario_analysis(analysis)

        # è¼¸å‡ºç¸½é«”åˆ†æ
        self._print_overall_analysis(all_analyses)

        return all_analyses

    def _print_scenario_analysis(self, analysis: Dict[str, Any]):
        """è¼¸å‡ºå–®å€‹å ´æ™¯çš„åˆ†æçµæœ"""
        print(f"\nğŸ“Š {analysis['scenario']} åˆ†æçµæœ:")
        print(f"å¹³å‡å…§å®¹å¢åŠ : {analysis['summary']['avg_content_increase']:.1f}%")
        print(f"å¹³å‡ç‰‡æ®µå¢åŠ : {analysis['summary']['avg_chunk_increase']:.1f}%")

        print("\næŸ¥è©¢è©³ç´°å°æ¯”:")
        for query, comp in analysis['query_comparisons'].items():
            print(f"\nâ“ {query}")
            print(f"  å‚³çµ±RAG: {comp['traditional']['length']}å­—ç¬¦, {comp['traditional']['chunks']}ç‰‡æ®µ")
            print(f"  æ–‡ç« æ„ŸçŸ¥: {comp['enhanced']['length']}å­—ç¬¦, {comp['enhanced']['chunks']}ç‰‡æ®µ")
            print(f"  å…§å®¹è±å¯Œåº¦æå‡: {comp['improvement']['content_richness']:.1f}å€")

    def _print_overall_analysis(self, all_analyses: List[Dict[str, Any]]):
        """è¼¸å‡ºç¸½é«”åˆ†æçµæœ"""
        print("\n" + "=" * 60)
        print("ğŸ† ç¸½é«”å°æ¯”åˆ†æçµæœ")
        print("=" * 60)

        total_scenarios = len(all_analyses)
        avg_content_increase = sum(a['summary']['avg_content_increase'] for a in all_analyses) / total_scenarios
        avg_chunk_increase = sum(a['summary']['avg_chunk_increase'] for a in all_analyses) / total_scenarios

        print(f"\nğŸ“ˆ æ•´é«”æ”¹å–„å¹…åº¦:")
        print(f"å¹³å‡å…§å®¹è±å¯Œåº¦æå‡: {avg_content_increase:.1f}%")
        print(f"å¹³å‡ä¸Šä¸‹æ–‡ç‰‡æ®µå¢åŠ : {avg_chunk_increase:.1f}%")

        print(f"\nğŸ¯ è²¡ç¶“æ–°èç†è§£å„ªå‹¢:")
        print("âœ… å®Œæ•´æŠ•è³‡é‚è¼¯éˆæ¢ (è²¡å ±â†’åˆ†æâ†’å¸‚å ´åæ‡‰)")
        print("âœ… å¤šç¶­åº¦é¢¨éšªåˆ†æ (åŸºæœ¬é¢+æŠ€è¡“é¢+æ¶ˆæ¯é¢)")
        print("âœ… å› æœé—œä¿‚ç†è§£ (äº‹ä»¶â†’å½±éŸ¿â†’é æ¸¬)")
        print("âœ… æ™‚é–“åºåˆ—é€£è²«æ€§ (æ­·å²â†’ç¾åœ¨â†’æœªä¾†)")

        print(f"\nâš–ï¸ æˆæœ¬æ•ˆç›Šåˆ†æ:")
        enhanced_avg_length = sum(a['summary']['enhanced_avg_length'] for a in all_analyses) / total_scenarios
        traditional_avg_length = sum(a['summary']['traditional_avg_length'] for a in all_analyses) / total_scenarios

        print(f"å‚³çµ±RAGå¹³å‡å›æ‡‰é•·åº¦: {traditional_avg_length:.0f} å­—ç¬¦")
        print(f"æ–‡ç« æ„ŸçŸ¥RAGå¹³å‡å›æ‡‰é•·åº¦: {enhanced_avg_length:.0f} å­—ç¬¦")
        print(f"Tokenæˆæœ¬å¢åŠ ä¼°ç®—: {(enhanced_avg_length / traditional_avg_length - 1) * 100:.1f}%")

        print(f"\nğŸ’¡ å»ºè­°:")
        if avg_content_increase > 100:
            print("ğŸŸ¢ å¼·çƒˆå»ºè­°ä½¿ç”¨æ–‡ç« æ„ŸçŸ¥RAG - å…§å®¹è±å¯Œåº¦å¤§å¹…æå‡")
        elif avg_content_increase > 50:
            print("ğŸŸ¡ å»ºè­°ä½¿ç”¨æ–‡ç« æ„ŸçŸ¥RAG - å…§å®¹è±å¯Œåº¦é¡¯è‘—æå‡")
        else:
            print("ğŸŸ  éœ€è¦è©•ä¼°æˆæœ¬æ•ˆç›Š - æå‡å¹…åº¦æœ‰é™")


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    tester = RAGComparisonTester()
    await tester.run_comprehensive_test()


if __name__ == "__main__":
    asyncio.run(main())