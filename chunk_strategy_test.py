#!/usr/bin/env python3
"""
æ¸¬è©¦ä¸åŒåˆ‡å¡Šç­–ç•¥çš„æ•ˆæœ
"""

import json
import re
from typing import List, Dict, Any, Tuple
from pathlib import Path

def strategy_1_fixed_size(text: str, chunk_size: int = 300) -> List[str]:
    """ç­–ç•¥1ï¼šå›ºå®šå¤§å°åˆ‡å¡Šï¼ˆç°¡å–®ç²—æš´ï¼‰"""
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunks.append(text[i:i + chunk_size])
    return chunks

def strategy_2_sentence_aware(text: str, chunk_size: int = 400, overlap: int = 50) -> List[str]:
    """ç­–ç•¥2ï¼šå¥å­æ„ŸçŸ¥åˆ‡å¡Šï¼ˆä¿æŒèªç¾©å®Œæ•´ï¼‰"""
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)

    current_chunk = ""
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue

        sentence += "ã€‚"

        if len(current_chunk + sentence) <= chunk_size:
            current_chunk += sentence
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence if overlap == 0 else current_chunk[-overlap:] + sentence
            else:
                chunks.append(sentence[:chunk_size])
                current_chunk = sentence[chunk_size:]

    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks

def strategy_3_semantic_sections(text: str, chunk_size: int = 450) -> List[str]:
    """ç­–ç•¥3ï¼šèªç¾©æ®µè½åˆ‡å¡Šï¼ˆåŸºæ–¼å…§å®¹çµæ§‹ï¼‰"""
    # è­˜åˆ¥æ®µè½åˆ†éš”ç¬¦
    paragraphs = re.split(r'\n\s*\n', text)

    chunks = []
    current_chunk = ""

    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if not paragraph:
            continue

        # å¦‚æœå–®å€‹æ®µè½å¤ªé•·ï¼Œéœ€è¦é€²ä¸€æ­¥åˆ‡åˆ†
        if len(paragraph) > chunk_size:
            # ä½¿ç”¨å¥å­æ„ŸçŸ¥æ–¹å¼åˆ‡åˆ†é•·æ®µè½
            if current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = ""

            para_chunks = strategy_2_sentence_aware(paragraph, chunk_size, 30)
            chunks.extend(para_chunks)
        else:
            # æª¢æŸ¥æ˜¯å¦å¯ä»¥å’Œç•¶å‰ chunk åˆä½µ
            if len(current_chunk + "\n" + paragraph) <= chunk_size:
                current_chunk += "\n" + paragraph if current_chunk else paragraph
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph

    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks

def strategy_4_content_aware(text: str, chunk_size: int = 500) -> List[str]:
    """ç­–ç•¥4ï¼šå…§å®¹æ„ŸçŸ¥åˆ‡å¡Šï¼ˆé‡å°è²¡ç¶“æ–°èå„ªåŒ–ï¼‰"""
    # è²¡ç¶“æ–°èçš„ç‰¹æ®Šæ¨™è¨˜
    financial_markers = [
        r'[è‚¡ç¥¨ä»£ç¢¼|è­‰åˆ¸ä»£ç¢¼][:ï¼š]\s*\d+',
        r'æ”¶ç›¤åƒ¹|é–‹ç›¤åƒ¹|æœ€é«˜åƒ¹|æœ€ä½åƒ¹',
        r'æ¼²è·Œ[å¹…åº¦]*[:ï¼š]',
        r'æˆäº¤é‡|æˆäº¤é¡',
        r'å¸‚å€¼|æ·¨å€¼|æœ¬ç›Šæ¯”|è‚¡åƒ¹æ·¨å€¼æ¯”',
        r'ç‡Ÿæ”¶|ç²åˆ©|EPS|ROE|ROA',
        r'åˆ†æå¸«|æŠ•è³‡å»ºè­°|ç›®æ¨™åƒ¹',
        r'é¢¨éšª[æé†’|è­¦ç¤º|è©•ä¼°]',
    ]

    chunks = []

    # å…ˆæŒ‰æ®µè½åˆ†å‰²
    paragraphs = text.split('\n')
    current_chunk = ""

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue

        # æª¢æŸ¥æ˜¯å¦åŒ…å«é‡è¦è²¡ç¶“ä¿¡æ¯
        has_financial_info = any(re.search(pattern, para) for pattern in financial_markers)

        # å¦‚æœåŒ…å«é‡è¦è²¡ç¶“ä¿¡æ¯ï¼Œå„ªå…ˆä¿æŒå®Œæ•´
        if has_financial_info and len(para) <= chunk_size * 1.2:  # å…è¨±ç¨å¾®è¶…å‡º
            if len(current_chunk + "\n" + para) > chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = para
            else:
                current_chunk += "\n" + para if current_chunk else para
        else:
            # æ™®é€šæ®µè½æŒ‰æ¨™æº–è¦å‰‡è™•ç†
            if len(current_chunk + "\n" + para) <= chunk_size:
                current_chunk += "\n" + para if current_chunk else para
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())

                # å¦‚æœæ®µè½å¤ªé•·ï¼Œä½¿ç”¨å¥å­æ„ŸçŸ¥åˆ‡åˆ†
                if len(para) > chunk_size:
                    para_chunks = strategy_2_sentence_aware(para, chunk_size, 30)
                    chunks.extend(para_chunks)
                    current_chunk = ""
                else:
                    current_chunk = para

    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks

def strategy_5_hierarchical(text: str, min_chunk: int = 200, max_chunk: int = 600) -> List[str]:
    """ç­–ç•¥5ï¼šéšå±¤å¼åˆ‡å¡Šï¼ˆå‹•æ…‹èª¿æ•´å¤§å°ï¼‰"""
    # æª¢æ¸¬æ–‡æœ¬é¡å‹å’Œé‡è¦æ€§
    def get_content_importance(content: str) -> float:
        financial_keywords = ['æŠ•è³‡', 'è‚¡ç¥¨', 'å¸‚å ´', 'åˆ†æ', 'å»ºè­°', 'é¢¨éšª', 'å ±é…¬', 'åƒ¹æ ¼']
        importance = sum(1 for keyword in financial_keywords if keyword in content)
        return min(importance / len(financial_keywords), 1.0)

    chunks = []
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)

    current_chunk = ""
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue

        sentence += "ã€‚"
        importance = get_content_importance(sentence)

        # æ ¹æ“šé‡è¦æ€§å‹•æ…‹èª¿æ•´ç›®æ¨™å¤§å°
        target_size = min_chunk + int((max_chunk - min_chunk) * importance)

        if len(current_chunk + sentence) <= target_size:
            current_chunk += sentence
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence

    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks

def analyze_chunk_quality(chunks: List[str], strategy_name: str) -> Dict[str, Any]:
    """åˆ†æåˆ‡å¡Šè³ªé‡"""
    if not chunks:
        return {"strategy": strategy_name, "error": "No chunks generated"}

    chunk_lengths = [len(chunk) for chunk in chunks]

    # æª¢æŸ¥èªç¾©å®Œæ•´æ€§ï¼ˆç°¡å–®æŒ‡æ¨™ï¼‰
    incomplete_chunks = sum(1 for chunk in chunks if not chunk.strip().endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', ':', 'ï¼š')))

    # æª¢æŸ¥ä¿¡æ¯å¯†åº¦
    financial_keywords = ['æŠ•è³‡', 'è‚¡ç¥¨', 'å¸‚å ´', 'åˆ†æ', 'å»ºè­°', 'é¢¨éšª', 'å ±é…¬', 'åƒ¹æ ¼', 'æ”¶ç›¤', 'é–‹ç›¤']
    avg_keywords_per_chunk = sum(
        sum(1 for keyword in financial_keywords if keyword in chunk)
        for chunk in chunks
    ) / len(chunks)

    return {
        "strategy": strategy_name,
        "total_chunks": len(chunks),
        "avg_length": sum(chunk_lengths) / len(chunks),
        "min_length": min(chunk_lengths),
        "max_length": max(chunk_lengths),
        "length_std": (sum((l - sum(chunk_lengths)/len(chunks))**2 for l in chunk_lengths) / len(chunks))**0.5,
        "incomplete_chunks": incomplete_chunks,
        "completeness_rate": (len(chunks) - incomplete_chunks) / len(chunks),
        "avg_keywords_per_chunk": avg_keywords_per_chunk,
        "information_density": avg_keywords_per_chunk / (sum(chunk_lengths) / len(chunks) / 100)  # æ¯100å­—ç¬¦çš„é—œéµå­—å¯†åº¦
    }

def test_all_strategies():
    """æ¸¬è©¦æ‰€æœ‰åˆ‡å¡Šç­–ç•¥"""
    # è¼‰å…¥æ¸¬è©¦æ•¸æ“š
    data_file = Path(__file__).parent / "data" / "real_cnyes_news_20250927_132114.json"

    if not data_file.exists():
        print(f"æ¸¬è©¦æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return

    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    articles = data.get('articles', [])
    if not articles:
        print("æ²’æœ‰æ‰¾åˆ°æ–‡ç« æ•¸æ“š")
        return

    print("=== åˆ‡å¡Šç­–ç•¥æ•ˆæœåˆ†æ ===\n")

    # é¸æ“‡ä¸€ç¯‡ä»£è¡¨æ€§æ–‡ç« é€²è¡Œæ¸¬è©¦
    test_article = articles[0]  # é¸æ“‡ç¬¬ä¸€ç¯‡æ–‡ç« 
    title = test_article.get('title', '')
    content = test_article.get('content', '')
    category = test_article.get('category', '')

    full_text = f"æ¨™é¡Œï¼š{title}\nåˆ†é¡ï¼š{category}\nå…§å®¹ï¼š{content}"

    print(f"æ¸¬è©¦æ–‡ç« ï¼š{title[:50]}...")
    print(f"åŸæ–‡é•·åº¦ï¼š{len(full_text)} å­—ç¬¦\n")

    strategies = [
        ("å›ºå®šå¤§å°åˆ‡å¡Š", lambda t: strategy_1_fixed_size(t, 300)),
        ("å¥å­æ„ŸçŸ¥åˆ‡å¡Š", lambda t: strategy_2_sentence_aware(t, 400, 50)),
        ("èªç¾©æ®µè½åˆ‡å¡Š", lambda t: strategy_3_semantic_sections(t, 450)),
        ("å…§å®¹æ„ŸçŸ¥åˆ‡å¡Š", lambda t: strategy_4_content_aware(t, 500)),
        ("éšå±¤å¼åˆ‡å¡Š", lambda t: strategy_5_hierarchical(t, 200, 600))
    ]

    results = []

    for strategy_name, strategy_func in strategies:
        print(f"--- {strategy_name} ---")

        try:
            chunks = strategy_func(full_text)
            analysis = analyze_chunk_quality(chunks, strategy_name)
            results.append(analysis)

            print(f"å¡Šæ•¸é‡: {analysis['total_chunks']}")
            print(f"å¹³å‡é•·åº¦: {analysis['avg_length']:.1f} å­—ç¬¦")
            print(f"é•·åº¦ç¯„åœ: {analysis['min_length']}-{analysis['max_length']}")
            print(f"å®Œæ•´æ€§: {analysis['completeness_rate']:.1%}")
            print(f"ä¿¡æ¯å¯†åº¦: {analysis['information_density']:.3f}")
            print(f"ç¤ºä¾‹å¡Š: {chunks[0][:100]}...")
            print()

        except Exception as e:
            print(f"ç­–ç•¥åŸ·è¡Œå¤±æ•—: {e}")
            print()

    # ç¸½çµæœ€ä½³ç­–ç•¥
    print("=== ç­–ç•¥æ’å ===")

    # ç¶œåˆè©•åˆ†ï¼ˆå®Œæ•´æ€§ 40% + ä¿¡æ¯å¯†åº¦ 40% + å¡Šæ•¸é‡é©ä¸­ 20%ï¼‰
    for result in results:
        if 'error' not in result:
            completeness_score = result['completeness_rate']
            density_score = min(result['information_density'] / 0.1, 1.0)  # æ­£è¦åŒ–åˆ°0-1
            chunk_count_score = 1.0 / (1.0 + abs(result['total_chunks'] - 5) * 0.1)  # ç†æƒ³å¡Šæ•¸ç´„5å€‹

            final_score = (completeness_score * 0.4 +
                          density_score * 0.4 +
                          chunk_count_score * 0.2)

            result['final_score'] = final_score

    # æ’åºä¸¦é¡¯ç¤ºçµæœ
    valid_results = [r for r in results if 'error' not in r]
    valid_results.sort(key=lambda x: x['final_score'], reverse=True)

    for i, result in enumerate(valid_results, 1):
        print(f"{i}. {result['strategy']} - ç¸½åˆ†: {result['final_score']:.3f}")
        print(f"   å®Œæ•´æ€§: {result['completeness_rate']:.1%}, ä¿¡æ¯å¯†åº¦: {result['information_density']:.3f}")

    print(f"\nğŸ† æ¨è–¦ç­–ç•¥: {valid_results[0]['strategy']}")

    return valid_results[0]['strategy']

if __name__ == "__main__":
    best_strategy = test_all_strategies()